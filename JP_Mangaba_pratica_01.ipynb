{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtKtaDlnPwx0j12VSBd0eR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaopaulonasc/CiscoPacketTracer/blob/main/JP_Mangaba_pratica_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE5LORdpfcEy",
        "outputId": "6d9f4baf-cdf4-46dc-8df2-0e9b8146200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mangaba_ai'...\n",
            "remote: Enumerating objects: 448, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 448 (delta 67), reused 119 (delta 52), pack-reused 287 (from 1)\u001b[K\n",
            "Receiving objects: 100% (448/448), 3.14 MiB | 8.75 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mangaba-ai/mangaba_ai.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mangaba_ai\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMzBqFFmghQZ",
        "outputId": "5b8a600e-0608-4c06-8784-f2a613d39c79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mangaba_ai\n",
            "Processing /content/mangaba_ai\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-generativeai>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from mangaba-ai==1.0.0) (0.8.5)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mangaba-ai==1.0.0) (1.1.1)\n",
            "Collecting loguru>=0.6.0 (from mangaba-ai==1.0.0)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from mangaba-ai==1.0.0) (2.11.7)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from mangaba-ai==1.0.0) (2.32.4)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.12/dist-packages (from mangaba-ai==1.0.0) (15.0.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.3.0->mangaba-ai==1.0.0) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (1.26.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->mangaba-ai==1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->mangaba-ai==1.0.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->mangaba-ai==1.0.0) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->mangaba-ai==1.0.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->mangaba-ai==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->mangaba-ai==1.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->mangaba-ai==1.0.0) (2025.8.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.0->mangaba-ai==1.0.0) (0.6.1)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mangaba-ai\n",
            "  Building wheel for mangaba-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mangaba-ai: filename=mangaba_ai-1.0.0-py3-none-any.whl size=28120 sha256=6e97d53784310e64e017f0f704b58581f9149c211d7577c15627d1196b5599c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h0kouj8j/wheels/a5/a6/d9/d58997caf82077818d9e6eddd6ed270205698abebad944be1a\n",
            "Successfully built mangaba-ai\n",
            "Installing collected packages: loguru, mangaba-ai\n",
            "Successfully installed loguru-0.7.3 mangaba-ai-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "PVQF1U5qhPAN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mangaba_ai import MangabaAgent"
      ],
      "metadata": {
        "id": "-5Em0TH1hgiY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Criação de uma classe de assistente inteligente com histórico persistente\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class AssistenteInteligente:\n",
        "    def __init__(self, agent_id):\n",
        "        self.agente = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "        self.historico = []\n",
        "\n",
        "    def interagir(self, mensagem):\n",
        "        self.historico.append(mensagem)\n",
        "        resposta = self.agente.chat(mensagem)\n",
        "        self.historico.append(resposta)\n",
        "        return resposta\n",
        "\n",
        "assistente = AssistenteInteligente(agent_id=\"assistente1\")\n",
        "print(assistente.interagir(\"Explique a iniciativa Mangaba AI de Sergipe\"))\n",
        "print(assistente.interagir(\"Faça um resumo em tópicos da explicação anterior.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "-qntmFmthjZn",
        "outputId": "195a54c1-bc0e-4b41-9faf-5604127ddcd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m20:15:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[assistente1]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: assistente1, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m20:15:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[assistente1]\u001b[0m | \u001b[1m💬 Chat: Explique a iniciativa Mangaba AI de Sergipe... → A iniciativa **Mangaba AI de Sergipe** é um projet...\u001b[0m\n",
            "A iniciativa **Mangaba AI de Sergipe** é um projeto ambicioso e pioneiro que busca posicionar o estado de Sergipe como um polo de inovação e desenvolvimento em Inteligência Artificial (IA), com um forte foco na aplicação dessa tecnologia para impulsionar a economia local, promover a sustentabilidade e qualificar a sua força de trabalho.\n",
            "\n",
            "O nome \"Mangaba AI\" é simbólico: a **mangaba** é um fruto nativo e representativo do Nordeste brasileiro, incluindo Sergipe, remetendo à identidade local e à riqueza natural do estado. \"AI\" (Artificial Intelligence) aponta para a modernidade, a tecnologia de ponta e o futuro. Juntos, eles representam a união entre a vocação natural e cultural de Sergipe e o potencial transformador da tecnologia.\n",
            "\n",
            "**Principais Pilares e Objetivos:**\n",
            "\n",
            "1.  **Desenvolvimento de Talentos em IA:**\n",
            "    *   Fomentar a formação e capacitação de profissionais em IA, desde o ensino básico até o superior e pós-graduação.\n",
            "    *   Parcerias com universidades (como a UFS - Universidade Federal de Sergipe e a UNIT - Universidade Tiradentes) e instituições de ensino técnico para criar cursos, programas de residência e trilhas de aprendizado em áreas como ciência de dados, machine learning, visão computacional, entre outras.\n",
            "\n",
            "2.  **IA para o Agronegócio (Agro 4.0):**\n",
            "    *   Uma das aplicações mais importantes é no setor agrícola, otimizando a produção, monitorando culturas (inclusive a mangaba, de forma simbólica e prática), prevendo pragas e doenças, e gerenciando recursos hídricos e solos de forma mais eficiente.\n",
            "    *   Uso de sensores, drones e análise de dados para aumentar a produtividade e a sustentabilidade no campo sergipano.\n",
            "\n",
            "3.  **Fomento ao Ecossistema de Inovação:**\n",
            "    *   Incentivar a criação e o desenvolvimento de startups de IA em Sergipe.\n",
            "    *   Promover a colaboração entre a academia, o setor privado e o governo para gerar soluções inovadoras e aplicáveis às necessidades do estado.\n",
            "    *   Atração de investimentos e empresas de tecnologia para Sergipe.\n",
            "\n",
            "4.  **IA para a Gestão Pública e Serviços Inteligentes:**\n",
            "    *   Desenvolver soluções de IA para melhorar a eficiência da gestão pública, otimizar serviços ao cidadão, planejar políticas públicas baseadas em dados e impulsionar o conceito de \"cidades inteligentes\".\n",
            "\n",
            "5.  **Sustentabilidade e Monitoramento Ambiental:**\n",
            "    *   Aplicar IA para monitorar ecossistemas, prever desastres naturais, gerenciar resíduos e apoiar a conservação ambiental, alinhando o desenvolvimento tecnológico com a preservação dos recursos naturais de Sergipe.\n",
            "\n",
            "**Importância e Impacto Esperado:**\n",
            "\n",
            "*   **Diversificação Econômica:** Reduzir a dependência de setores tradicionais e criar novas fontes de riqueza.\n",
            "*   **Geração de Empregos de Alto Valor:** Criar oportunidades para profissionais qualificados em tecnologia, atraindo e retendo talentos no estado.\n",
            "*   **Competitividade:** Posicionar Sergipe na vanguarda da tecnologia e inovação no cenário nacional e internacional.\n",
            "*   **Melhora na Qualidade de Vida:** Através de serviços públicos mais eficientes, um agronegócio mais sustentável e um ambiente de negócios mais dinâmico.\n",
            "\n",
            "Em resumo, a iniciativa Mangaba AI é a aposta estratégica de Sergipe na inteligência artificial como motor de transformação econômica, social e ambiental, utilizando sua identidade regional como inspiração para construir um futuro mais tecnológico e próspero.\n",
            "\u001b[32m20:15:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[assistente1]\u001b[0m | \u001b[1m💬 Chat: Faça um resumo em tópicos da explicação anterior.... → Com certeza! Aqui está um resumo em tópicos da ini...\u001b[0m\n",
            "Com certeza! Aqui está um resumo em tópicos da iniciativa Mangaba AI de Sergipe:\n",
            "\n",
            "*   **Visão Geral:** Projeto pioneiro para posicionar Sergipe como polo de inovação em Inteligência Artificial (IA), focando na economia local, sustentabilidade e qualificação da força de trabalho.\n",
            "*   **Nome Simbólico:**\n",
            "    *   **Mangaba:** Fruto nativo de Sergipe, remetendo à identidade local e riqueza natural.\n",
            "    *   **AI:** Representa modernidade, tecnologia de ponta e futuro.\n",
            "    *   Juntos, simbolizam a união da vocação cultural com o potencial tecnológico.\n",
            "*   **Principais Pilares e Objetivos:**\n",
            "    1.  **Desenvolvimento de Talentos em IA:** Fomentar formação desde o ensino básico ao superior, com parcerias (UFS, UNIT).\n",
            "    2.  **IA para o Agronegócio (Agro 4.0):** Otimizar produção, monitorar culturas, prever pragas, gerenciar recursos hídricos e solos com sensores, drones e análise de dados.\n",
            "    3.  **Fomento ao Ecossistema de Inovação:** Incentivar startups de IA, promover colaboração (academia, setor privado, governo) e atrair investimentos.\n",
            "    4.  **IA para a Gestão Pública e Serviços Inteligentes:** Melhorar eficiência governamental, otimizar serviços ao cidadão e impulsionar \"cidades inteligentes\".\n",
            "    5.  **Sustentabilidade e Monitoramento Ambiental:** Aplicar IA para monitorar ecossistemas, prever desastres, gerenciar resíduos e apoiar a conservação.\n",
            "*   **Importância e Impacto Esperado:**\n",
            "    *   **Diversificação Econômica:** Novas fontes de riqueza, reduzindo dependência de setores tradicionais.\n",
            "    *   **Geração de Empregos de Alto Valor:** Oportunidades para profissionais qualificados, atraindo e retendo talentos.\n",
            "    *   **Competitividade:** Sergipe na vanguarda da tecnologia nacional e internacional.\n",
            "    *   **Melhora na Qualidade de Vida:** Serviços públicos mais eficientes, agronegócio sustentável e ambiente de negócios dinâmico.\n",
            "*   **Em Resumo:** Aposta estratégica de Sergipe na IA como motor de transformação econômica, social e ambiental, com inspiração na identidade regional.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A iniciativa **Mangaba AI de Sergipe** é um projeto ambicioso e pioneiro que busca posicionar o estado de Sergipe como um polo de inovação e desenvolvimento em Inteligência Artificial (IA), com um forte foco na aplicação dessa tecnologia para impulsionar a economia local, promover a sustentabilidade e qualificar a sua força de trabalho.\n",
        "\n",
        "O nome \"Mangaba AI\" é simbólico: a **mangaba** é um fruto nativo e representativo do Nordeste brasileiro, incluindo Sergipe, remetendo à identidade local e à riqueza natural do estado. \"AI\" (Artificial Intelligence) aponta para a modernidade, a tecnologia de ponta e o futuro. Juntos, eles representam a união entre a vocação natural e cultural de Sergipe e o potencial transformador da tecnologia.\n",
        "\n",
        "**Principais Pilares e Objetivos:**\n",
        "\n",
        "1.  **Desenvolvimento de Talentos em IA:**\n",
        "    *   Fomentar a formação e capacitação de profissionais em IA, desde o ensino básico até o superior e pós-graduação.\n",
        "    *   Parcerias com universidades (como a UFS - Universidade Federal de Sergipe e a UNIT - Universidade Tiradentes) e instituições de ensino técnico para criar cursos, programas de residência e trilhas de aprendizado em áreas como ciência de dados, machine learning, visão computacional, entre outras.\n",
        "\n",
        "2.  **IA para o Agronegócio (Agro 4.0):**\n",
        "    *   Uma das aplicações mais importantes é no setor agrícola, otimizando a produção, monitorando culturas (inclusive a mangaba, de forma simbólica e prática), prevendo pragas e doenças, e gerenciando recursos hídricos e solos de forma mais eficiente.\n",
        "    *   Uso de sensores, drones e análise de dados para aumentar a produtividade e a sustentabilidade no campo sergipano.\n",
        "\n",
        "3.  **Fomento ao Ecossistema de Inovação:**\n",
        "    *   Incentivar a criação e o desenvolvimento de startups de IA em Sergipe.\n",
        "    *   Promover a colaboração entre a academia, o setor privado e o governo para gerar soluções inovadoras e aplicáveis às necessidades do estado.\n",
        "    *   Atração de investimentos e empresas de tecnologia para Sergipe.\n",
        "\n",
        "4.  **IA para a Gestão Pública e Serviços Inteligentes:**\n",
        "    *   Desenvolver soluções de IA para melhorar a eficiência da gestão pública, otimizar serviços ao cidadão, planejar políticas públicas baseadas em dados e impulsionar o conceito de \"cidades inteligentes\".\n",
        "\n",
        "5.  **Sustentabilidade e Monitoramento Ambiental:**\n",
        "    *   Aplicar IA para monitorar ecossistemas, prever desastres naturais, gerenciar resíduos e apoiar a conservação ambiental, alinhando o desenvolvimento tecnológico com a preservação dos recursos naturais de Sergipe.\n",
        "\n",
        "**Importância e Impacto Esperado:**\n",
        "\n",
        "*   **Diversificação Econômica:** Reduzir a dependência de setores tradicionais e criar novas fontes de riqueza.\n",
        "*   **Geração de Empregos de Alto Valor:** Criar oportunidades para profissionais qualificados em tecnologia, atraindo e retendo talentos no estado.\n",
        "*   **Competitividade:** Posicionar Sergipe na vanguarda da tecnologia e inovação no cenário nacional e internacional.\n",
        "*   **Melhora na Qualidade de Vida:** Através de serviços públicos mais eficientes, um agronegócio mais sustentável e um ambiente de negócios mais dinâmico.\n",
        "\n",
        "---\n",
        "\n",
        "*   **Visão Geral:** Projeto pioneiro para posicionar Sergipe como polo de inovação em Inteligência Artificial (IA), focando na economia local, sustentabilidade e qualificação da força de trabalho.\n",
        "*   **Nome Simbólico:**\n",
        "    *   **Mangaba:** Fruto nativo de Sergipe, remetendo à identidade local e riqueza natural.\n",
        "    *   **AI:** Representa modernidade, tecnologia de ponta e futuro.\n",
        "    *   Juntos, simbolizam a união da vocação cultural com o potencial tecnológico.\n",
        "*   **Principais Pilares e Objetivos:**\n",
        "    1.  **Desenvolvimento de Talentos em IA:** Fomentar formação desde o ensino básico ao superior, com parcerias (UFS, UNIT).\n",
        "    2.  **IA para o Agronegócio (Agro 4.0):** Otimizar produção, monitorar culturas, prever pragas, gerenciar recursos hídricos e solos com sensores, drones e análise de dados.\n",
        "    3.  **Fomento ao Ecossistema de Inovação:** Incentivar startups de IA, promover colaboração (academia, setor privado, governo) e atrair investimentos.\n",
        "    4.  **IA para a Gestão Pública e Serviços Inteligentes:** Melhorar eficiência governamental, otimizar serviços ao cidadão e impulsionar \"cidades inteligentes\".\n",
        "    5.  **Sustentabilidade e Monitoramento Ambiental:** Aplicar IA para monitorar ecossistemas, prever desastres, gerenciar resíduos e apoiar a conservação.\n",
        "*   **Importância e Impacto Esperado:**\n",
        "    *   **Diversificação Econômica:** Novas fontes de riqueza, reduzindo dependência de setores tradicionais.\n",
        "    *   **Geração de Empregos de Alto Valor:** Oportunidades para profissionais qualificados, atraindo e retendo talentos.\n",
        "    *   **Competitividade:** Sergipe na vanguarda da tecnologia nacional e internacional.\n",
        "    *   **Melhora na Qualidade de Vida:** Serviços públicos mais eficientes, agronegócio sustentável e ambiente de negócios dinâmico.\n",
        "*   **Em Resumo:** Aposta estratégica de Sergipe na IA como motor de transformação econômica, social e ambiental, com inspiração na identidade regional."
      ],
      "metadata": {
        "id": "nf8nevBijxai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Classe para sumarização automática de textos longos usando MangabaAgent\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class ResumidorDeTextos:\n",
        "    def __init__(self, agent_id=\"resumidor\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def resumir(self, texto, frases=3):\n",
        "        prompt = f\"Resuma o texto abaixo em {frases} frases:\\n{texto}\"\n",
        "        return self.agent.chat(prompt)\n",
        "\n",
        "texto_longo = \"\"\"Mangaba AI é um framework brasileiro open source para automação inteligente usando equipes de agentes autônomos, desenvolvido em Python. Ele foi criado pelo Dr. Dheiver Santos, especialista em Inteligência Artificial, com o foco em permitir que agentes especializados colaborem para resolver tarefas complexas de forma coordenada, eficiente e inteligente.[1][5]\n",
        "\n",
        "Principais características do Mangaba AI:\n",
        "\n",
        "- **Arquitetura multi-agente**: Permite criar equipes de agentes inteligentes, cada um com funções específicas (por exemplo, buscar informações, analisar dados, gerar relatórios), que atuam de forma colaborativa.[5][1]\n",
        "- **Memória contextual**: Os agentes compartilham histórico individual e coletivo para manter continuidade e melhorar os resultados.[1][5]\n",
        "- **Integração com modelos avançados**: Suporta nativamente integração com modelos Gemini (IA da Google) para ampliar as capacidades cognitivas dos agentes.[5][1]\n",
        "- **Ferramentas externas**: Pode acessar APIs e realizar buscas em tempo real (como via Google Search) para expandir a automação e o acesso à informação.[1][5]\n",
        "- **Gerenciamento inteligente de tarefas**: Organiza fluxos, dependências e prioridades automaticamente para garantir eficiência.[5][1]\n",
        "- **Processamento assíncrono**: Permite a execução paralela das tarefas, acelerando a resolução de problemas complexos.[1]\n",
        "- **API intuitiva**: Projetado com uma API simples e fácil de aprender, oferecendo exemplos claros para quem está iniciando com agentes autônomos.[1]\n",
        "\n",
        "### Aplicações práticas\n",
        "\n",
        "O Mangaba AI pode ser explorado em diversos setores, como:\n",
        "- Análise inteligente de documentos\n",
        "- Geração automatizada de relatórios\n",
        "- Pesquisa e desenvolvimento avançado\n",
        "- Automação de processos repetitivos\n",
        "- Criação de assistentes virtuais\n",
        "- Análise de sentimento em dados de clientes ou redes sociais\n",
        "- Automatização de processos financeiros (ex: categorização de gastos, previsão de fluxo de caixa, detecção de anomalias financeiras)[7]\n",
        "\n",
        "### Curiosidade sobre o nome\n",
        "\n",
        "O nome e a identidade visual do Mangaba AI são inspirados na fruta mangaba, nativa do Brasil, símbolo de versatilidade e capacidade de adaptação—valores que o framework busca refletir. As cores remetem ao ciclo natural da fruta, associando-se ao crescimento e evolução dos agentes de IA dentro do sistema.[5][1]\n",
        "\n",
        "### Aprendizado e comunidade\n",
        "\n",
        "O framework tem ganhado destaque na comunidade de IA pelo seu enfoque moderno e colaborativo, sendo tema de cursos práticos, palestras e discussões entre profissionais do setor de tecnologia, inovação e negócios.[2][9][5]\n",
        "\n",
        "Se você quer criar soluções modulares e escaláveis em IA, principalmente voltadas para automação corporativa, pesquisa ou workflows inteligentes, Mangaba AI é uma ótima opção para rápida prototipagem e projetos autônomos.[9][5][1]\n",
        "\"\"\"\n",
        "resumidor = ResumidorDeTextos()\n",
        "print(resumidor.resumir(texto_longo, frases=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0OrcpBCwlU25",
        "outputId": "dcacfb82-9bdd-43e1-d9bf-56f43cc6167c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m20:21:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[resumidor]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: resumidor, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m20:21:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[resumidor]\u001b[0m | \u001b[1m💬 Chat: Resuma o texto abaixo em 2 frases:\n",
            "Mangaba AI é um... → Mangaba AI é um framework brasileiro open source e...\u001b[0m\n",
            "Mangaba AI é um framework brasileiro open source em Python, desenvolvido pelo Dr. Dheiver Santos, que utiliza equipes de agentes autônomos para automação inteligente e resolução coordenada de tarefas complexas. Ele oferece uma arquitetura multi-agente com memória contextual e integração a modelos avançados, ideal para otimizar processos em análise de documentos, automação corporativa e pesquisa em diversos setores.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mangaba AI é um framework brasileiro open source em Python, desenvolvido pelo Dr. Dheiver Santos, que utiliza equipes de agentes autônomos para automação inteligente e resolução coordenada de tarefas complexas. Ele oferece uma arquitetura multi-agente com memória contextual e integração a modelos avançados, ideal para otimizar processos em análise de documentos, automação corporativa e pesquisa em diversos setores."
      ],
      "metadata": {
        "id": "S4vPQpXLmIE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Classe que integra busca, análise e geração de relatório estruturado\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class RelatorioIA:\n",
        "    def __init__(self, agent_id=\"relatorio\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id, tools=[\"search\"])\n",
        "\n",
        "    def gerar_relatorio(self, tema):\n",
        "        resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
        "        analise = self.agent.chat(f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\")\n",
        "        return analise\n",
        "\n",
        "relatorio = RelatorioIA()\n",
        "print(relatorio.gerar_relatorio(\"Impacto da IA na agricultura brasileira\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "y_b-ZQRemQN-",
        "outputId": "8fc62591-bb36-4b7f-8640-cc44c28fd65f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MangabaAgent.__init__() got an unexpected keyword argument 'tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3269312917.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manalise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrelatorio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRelatorioIA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelatorio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgerar_relatorio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Impacto da IA na agricultura brasileira\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3269312917.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_id)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRelatorioIA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relatorio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMangabaAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.5-flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"search\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgerar_relatorio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MangabaAgent.__init__() got an unexpected keyword argument 'tools'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Classe para revisão e sugestão de melhorias em códigos fornecidos\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class RevisorCodigo:\n",
        "    def __init__(self, agent_id=\"revisor\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def revisar(self, codigo):\n",
        "        prompt = f\"Revise o código abaixo e sugira melhorias seguindo boas práticas Python:\\n{codigo}\"\n",
        "        return self.agent.chat(prompt)\n",
        "\n",
        "codigo_exemplo = '''\n",
        "#Classe que integra busca, análise e geração de relatório estruturado\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class RelatorioIA:\n",
        "    def __init__(self, agent_id=\"relatorio\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id, tools=[\"search\"])\n",
        "\n",
        "    def gerar_relatorio(self, tema):\n",
        "        resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
        "        analise = self.agent.chat(f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\")\n",
        "        return analise\n",
        "\n",
        "relatorio = RelatorioIA()\n",
        "print(relatorio.gerar_relatorio(\"Impacto da IA na agricultura brasileira\"))\n",
        "'''\n",
        "revisor = RevisorCodigo()\n",
        "print(revisor.revisar(codigo_exemplo))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0sgILmdFrX9A",
        "outputId": "a2d5e674-b978-470e-ba12-b34813fb8b06"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m20:45:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[revisor]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: revisor, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m20:46:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[revisor]\u001b[0m | \u001b[1m💬 Chat: Revise o código abaixo e sugira melhorias seguindo... → O código é um bom ponto de partida para integrar u...\u001b[0m\n",
            "O código é um bom ponto de partida para integrar uma busca e geração de relatório com IA. No entanto, há várias melhorias que podem ser aplicadas seguindo boas práticas Python, especialmente em termos de robustez, flexibilidade, legibilidade e manutenibilidade.\n",
            "\n",
            "Aqui estão as sugestões, categorizadas para facilitar a revisão:\n",
            "\n",
            "---\n",
            "\n",
            "### **Revisão e Sugestões de Melhoria**\n",
            "\n",
            "#### 1. **Docstrings e Type Hinting**\n",
            "\n",
            "*   **Problema:** O código não possui docstrings nem type hints.\n",
            "*   **Melhoria:** Adicionar docstrings para a classe e seus métodos, explicando o propósito, parâmetros e retornos. Adicionar type hints para os argumentos dos métodos e retornos para melhorar a legibilidade e a detecção de erros.\n",
            "\n",
            "```python\n",
            "# ...\n",
            "class RelatorioIA:\n",
            "    \"\"\"\n",
            "    Classe para integrar busca, análise e geração de relatórios estruturados\n",
            "    utilizando um agente de IA (MangabaAgent).\n",
            "    \"\"\"\n",
            "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
            "        \"\"\"\n",
            "        Inicializa a classe RelatorioIA com um MangabaAgent configurado.\n",
            "\n",
            "        Args:\n",
            "            agent_id (str): ID único para o agente de IA.\n",
            "            model (str): Nome do modelo de IA a ser utilizado (e.g., \"gemini-2.5-flash\").\n",
            "            tools (list[str]): Lista de ferramentas a serem disponibilizadas para o agente.\n",
            "                                Padrão para [\"search\"] se não for fornecido.\n",
            "        \"\"\"\n",
            "        if tools is None:\n",
            "            tools = [\"search\"]\n",
            "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
            "\n",
            "    def gerar_relatorio(self, tema: str) -> str:\n",
            "        \"\"\"\n",
            "        Gera um relatório estruturado sobre um tema específico.\n",
            "\n",
            "        Realiza uma busca pelo tema, analisa os resultados e gera um relatório\n",
            "        crítico utilizando o agente de IA.\n",
            "\n",
            "        Args:\n",
            "            tema (str): O tópico ou tema sobre o qual o relatório será gerado.\n",
            "\n",
            "        Returns:\n",
            "            str: O relatório estruturado gerado pelo agente de IA.\n",
            "        \"\"\"\n",
            "        # ... (restante do método)\n",
            "```\n",
            "\n",
            "#### 2. **Flexibilidade e Configuração**\n",
            "\n",
            "*   **Problema:** O modelo de IA (`\"gemini-2.5-flash\"`) e as ferramentas (`[\"search\"]`) estão \"hardcoded\" no `__init__`.\n",
            "*   **Melhoria:** Permitir que o modelo e as ferramentas sejam configuráveis via parâmetros do construtor, com valores padrão razoáveis. Isso torna a classe mais reutilizável sem modificações diretas.\n",
            "\n",
            "```python\n",
            "# ... (ver exemplo acima no __init__)\n",
            "class RelatorioIA:\n",
            "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
            "        if tools is None:\n",
            "            tools = [\"search\"] # Garantir que o valor padrão seja um objeto mutável novo\n",
            "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
            "```\n",
            "\n",
            "#### 3. **Tratamento de Erros e Robustez**\n",
            "\n",
            "*   **Problema:** O código não lida com possíveis falhas na chamada `run_tool` ou `chat` (ex: erros de rede, API, resultados vazios).\n",
            "*   **Melhoria:**\n",
            "    *   Usar blocos `try-except` para capturar exceções das chamadas à API.\n",
            "    *   Verificar se os `resultados` da busca não estão vazios ou são inválidos antes de passá-los para a análise.\n",
            "    *   Retornar uma mensagem de erro ou lançar uma exceção personalizada em caso de falha.\n",
            "    *   Implementar logging para depuração e monitoramento.\n",
            "\n",
            "```python\n",
            "import logging\n",
            "# from mangaba_ai import MangabaAgent # Assumindo que MangabaAgent lança exceções específicas ou genéricas\n",
            "\n",
            "# Configuração básica de logging\n",
            "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
            "\n",
            "class RelatorioIA:\n",
            "    # ... (init conforme revisado)\n",
            "\n",
            "    def gerar_relatorio(self, tema: str) -> str:\n",
            "        logging.info(f\"Iniciando geração de relatório para o tema: '{tema}'\")\n",
            "        resultados = None\n",
            "        try:\n",
            "            logging.info(f\"Executando busca por: '{tema}'\")\n",
            "            resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
            "            if not resultados:\n",
            "                logging.warning(f\"A busca por '{tema}' não retornou resultados.\")\n",
            "                return \"Não foi possível encontrar dados relevantes para gerar o relatório.\"\n",
            "            logging.info(f\"Resultados da busca obtidos. Tamanho: {len(str(resultados))} caracteres.\")\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Erro ao executar a ferramenta 'search' para o tema '{tema}': {e}\")\n",
            "            return f\"Erro ao buscar dados: {e}\"\n",
            "\n",
            "        analise = None\n",
            "        try:\n",
            "            prompt = f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\"\n",
            "            logging.info(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
            "            # Considerar truncar 'resultados' se for muito longo para evitar limites de token\n",
            "            # ou dividir a tarefa para o agente.\n",
            "            analise = self.agent.chat(prompt)\n",
            "            if not analise:\n",
            "                logging.warning(\"O agente de IA não conseguiu gerar uma análise para os dados fornecidos.\")\n",
            "                return \"O agente de IA não conseguiu gerar um relatório.\"\n",
            "            logging.info(\"Relatório gerado com sucesso.\")\n",
            "            return analise\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Erro ao gerar a análise/relatório pelo agente de IA para o tema '{tema}': {e}\")\n",
            "            return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
            "\n",
            "```\n",
            "\n",
            "#### 4. **Logging**\n",
            "\n",
            "*   **Problema:** O código não possui nenhum mecanismo de logging.\n",
            "*   **Melhoria:** Integrar a biblioteca `logging` do Python para registrar o fluxo de execução, avisos e erros. Isso é fundamental para depuração e monitoramento em produção. (Já incluído no exemplo de Tratamento de Erros).\n",
            "\n",
            "#### 5. **Separação de Preocupações e Reusabilidade (Prompt Engineering)**\n",
            "\n",
            "*   **Problema:** O prompt para o agente está \"hardcoded\" dentro do método `gerar_relatorio`.\n",
            "*   **Melhoria:**\n",
            "    *   Definir o prompt como uma constante da classe ou um parâmetro, permitindo ajustes sem modificar a lógica do método.\n",
            "    *   Considerar que prompts podem se tornar complexos. Uma abordagem mais estruturada (e.g., templates de prompt, separação de instruções e dados) pode ser útil.\n",
            "    *   Adicionar parâmetros ao `chat` como `temperature`, `top_p`, etc., se o `MangabaAgent` os suportar, para controlar a criatividade e a diversidade das respostas.\n",
            "\n",
            "```python\n",
            "# ...\n",
            "class RelatorioIA:\n",
            "    DEFAULT_REPORT_PROMPT = \"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{data}\"\n",
            "    # ... (init)\n",
            "\n",
            "    def gerar_relatorio(self, tema: str, prompt_template: str = None, **chat_kwargs) -> str:\n",
            "        # ... (código existente)\n",
            "\n",
            "        if prompt_template is None:\n",
            "            prompt_template = self.DEFAULT_REPORT_PROMPT\n",
            "\n",
            "        # Adicione lógica para lidar com a substituição de {data} ou outro placeholder\n",
            "        # Aqui, {data} será substituído por resultados\n",
            "        final_prompt = prompt_template.format(data=resultados)\n",
            "\n",
            "        try:\n",
            "            logging.info(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
            "            # **chat_kwargs permitiria passar temperatura, top_p, etc., se suportado pelo MangabaAgent.chat()\n",
            "            analise = self.agent.chat(final_prompt, **chat_kwargs)\n",
            "            # ... (restante do código)\n",
            "```\n",
            "\n",
            "#### 6. **Gerenciamento de Tamanho de Entrada (Token Limits)**\n",
            "\n",
            "*   **Problema:** Os resultados da busca (`resultados`) podem ser muito longos, excedendo os limites de token do modelo de IA e/ou aumentando os custos.\n",
            "*   **Melhoria:** Implementar uma estratégia para gerenciar o tamanho dos `resultados` antes de passá-los para o `chat` do agente. Isso pode envolver:\n",
            "    *   **Truncagem:** Simplesmente cortar o texto após um certo número de caracteres/tokens.\n",
            "    *   **Resumo Prévio:** Usar o próprio agente (ou um agente menor/mais barato) para resumir os resultados da busca *antes* de enviá-los para a geração do relatório final.\n",
            "    *   **Processamento em Batches:** Dividir os resultados em partes e processá-las individualmente.\n",
            "\n",
            "```python\n",
            "# ... (dentro de gerar_relatorio, após obter resultados)\n",
            "MAX_RESULT_LENGTH = 8000  # Exemplo: Limite de caracteres para evitar estouro de tokens\n",
            "if len(str(resultados)) > MAX_RESULT_LENGTH:\n",
            "    logging.warning(f\"Resultados da busca excedem {MAX_RESULT_LENGTH} caracteres. Truncando.\")\n",
            "    # Uma forma simples de truncar (pode ser melhorado para truncar em limites de frase, etc.)\n",
            "    resultados_truncados = str(resultados)[:MAX_RESULT_LENGTH] + \" [TRUNCADO...]\"\n",
            "else:\n",
            "    resultados_truncados = resultados\n",
            "\n",
            "prompt = f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados_truncados}\"\n",
            "# ... (restante do método)\n",
            "```\n",
            "\n",
            "#### 7. **Considerações Adicionais (Async/Wait)**\n",
            "\n",
            "*   **Problema:** Se as chamadas à API `run_tool` e `chat` forem I/O-bound (o que é muito provável), o código atual é síncrono e bloqueará a execução.\n",
            "*   **Melhoria (futura):** Se `MangabaAgent` suportar chamadas assíncronas (e.g., `async_run_tool`, `async_chat`), considere refatorar a classe para usar `asyncio` e `await` para melhor desempenho em aplicações que precisam de concorrência.\n",
            "\n",
            "```python\n",
            "# Exemplo hipotético com async/await (requer MangabaAgent assíncrono)\n",
            "# class RelatorioIA:\n",
            "#     # ...\n",
            "#     async def gerar_relatorio_async(self, tema: str) -> str:\n",
            "#         try:\n",
            "#             resultados = await self.agent.async_run_tool(\"search\", parameters={\"query\": tema})\n",
            "#             if not resultados:\n",
            "#                 return \"Não foi possível encontrar dados relevantes.\"\n",
            "#         except Exception as e:\n",
            "#             return f\"Erro ao buscar dados: {e}\"\n",
            "#\n",
            "#         try:\n",
            "#             analise = await self.agent.async_chat(f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\")\n",
            "#             return analise\n",
            "#         except Exception as e:\n",
            "#             return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### **Código Final Sugerido (com as melhorias principais)**\n",
            "\n",
            "```python\n",
            "import logging\n",
            "from mangaba_ai import MangabaAgent # Assumindo que esta biblioteca está instalada e funcional\n",
            "\n",
            "# Configuração básica de logging\n",
            "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
            "\n",
            "class RelatorioIA:\n",
            "    \"\"\"\n",
            "    Classe para integrar busca, análise e geração de relatórios estruturados\n",
            "    utilizando um agente de IA (MangabaAgent).\n",
            "    \"\"\"\n",
            "    DEFAULT_REPORT_PROMPT_TEMPLATE = (\n",
            "        \"Com base nos dados fornecidos, analise criticamente as informações e gere um relatório estruturado.\\n\"\n",
            "        \"O relatório deve ser objetivo, cobrir os pontos principais e apresentar uma conclusão.\\n\"\n",
            "        \"Dados para análise:\\n{data}\"\n",
            "    )\n",
            "    MAX_SEARCH_RESULTS_TOKENS = 4000 # Exemplo: Limite de tokens para resultados de busca antes de enviar ao LLM\n",
            "\n",
            "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
            "        \"\"\"\n",
            "        Inicializa a classe RelatorioIA com um MangabaAgent configurado.\n",
            "\n",
            "        Args:\n",
            "            agent_id (str): ID único para o agente de IA.\n",
            "            model (str): Nome do modelo de IA a ser utilizado (e.g., \"gemini-2.5-flash\").\n",
            "            tools (list[str]): Lista de ferramentas a serem disponibilizadas para o agente.\n",
            "                                Padrão para [\"search\"] se não for fornecido.\n",
            "        \"\"\"\n",
            "        if tools is None:\n",
            "            tools = [\"search\"] # Garantir que o valor padrão seja um objeto mutável novo\n",
            "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
            "        logging.info(f\"RelatorioIA inicializado com agent_id='{agent_id}', model='{model}', tools={tools}\")\n",
            "\n",
            "    def gerar_relatorio(self, tema: str, prompt_template: str = None, **chat_kwargs) -> str:\n",
            "        \"\"\"\n",
            "        Gera um relatório estruturado sobre um tema específico.\n",
            "\n",
            "        Realiza uma busca pelo tema, analisa os resultados e gera um relatório\n",
            "        crítico utilizando o agente de IA.\n",
            "\n",
            "        Args:\n",
            "            tema (str): O tópico ou tema sobre o qual o relatório será gerado.\n",
            "            prompt_template (str, opcional): Template de string para o prompt do agente.\n",
            "                                             Deve conter '{data}' para a inserção dos resultados.\n",
            "                                             Usa DEFAULT_REPORT_PROMPT_TEMPLATE se None.\n",
            "            **chat_kwargs: Argumentos adicionais a serem passados para o método `self.agent.chat()`,\n",
            "                           como `temperature`, `top_p`, etc., se suportados pelo MangabaAgent.\n",
            "\n",
            "        Returns:\n",
            "            str: O relatório estruturado gerado pelo agente de IA, ou uma mensagem de erro.\n",
            "        \"\"\"\n",
            "        logging.info(f\"Iniciando geração de relatório para o tema: '{tema}'\")\n",
            "        raw_resultados = None\n",
            "\n",
            "        # 1. Executar a busca\n",
            "        try:\n",
            "            logging.debug(f\"Executando busca por: '{tema}' com a ferramenta 'search'.\")\n",
            "            raw_resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
            "\n",
            "            if not raw_resultados:\n",
            "                logging.warning(f\"A busca por '{tema}' não retornou resultados. Não é possível gerar o relatório.\")\n",
            "                return \"Não foi possível encontrar dados relevantes para gerar o relatório.\"\n",
            "\n",
            "            logging.info(f\"Resultados da busca obtidos. Tamanho: {len(str(raw_resultados))} caracteres.\")\n",
            "\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Erro ao executar a ferramenta 'search' para o tema '{tema}': {e}\", exc_info=True)\n",
            "            return f\"Erro ao buscar dados: {e}\"\n",
            "\n",
            "        # 2. Pré-processar resultados (gerenciar limites de tokens)\n",
            "        processed_resultados = str(raw_resultados)\n",
            "        # Assumindo uma função auxiliar para estimar tokens ou truncar\n",
            "        # Em um cenário real, você usaria um tokenizer específico do modelo\n",
            "        if len(processed_resultados.split()) > self.MAX_SEARCH_RESULTS_TOKENS:\n",
            "            logging.warning(f\"Resultados da busca excedem {self.MAX_SEARCH_RESULTS_TOKENS} tokens (aproximadamente). Truncando.\")\n",
            "            # Truncagem simples por palavras, pode ser refinado\n",
            "            processed_resultados = \" \".join(processed_resultados.split()[:self.MAX_SEARCH_RESULTS_TOKENS]) + \"\\n[...Dados truncados para caber no limite do modelo...]\"\n",
            "            logging.debug(f\"Resultados truncados. Novo tamanho: {len(processed_resultados)} caracteres.\")\n",
            "\n",
            "        # 3. Gerar o relatório com o agente de IA\n",
            "        analise = None\n",
            "        try:\n",
            "            current_prompt_template = prompt_template if prompt_template is not None else self.DEFAULT_REPORT_PROMPT_TEMPLATE\n",
            "            final_prompt = current_prompt_template.format(data=processed_resultados)\n",
            "\n",
            "            logging.debug(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
            "            analise = self.agent.chat(final_prompt, **chat_kwargs)\n",
            "\n",
            "            if not analise:\n",
            "                logging.warning(\"O agente de IA não conseguiu gerar uma análise/relatório significativo para os dados fornecidos.\")\n",
            "                return \"O agente de IA não conseguiu gerar um relatório com os dados disponíveis.\"\n",
            "\n",
            "            logging.info(\"Relatório gerado com sucesso.\")\n",
            "            return analise\n",
            "\n",
            "        except Exception as e:\n",
            "            logging.error(f\"Erro ao gerar a análise/relatório pelo agente de IA para o tema '{tema}': {e}\", exc_info=True)\n",
            "            return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
            "\n",
            "# --- Exemplo de Uso ---\n",
            "if __name__ == \"__main__\":\n",
            "    # Instanciando a classe com configurações padrão\n",
            "    relatorio_generator = RelatorioIA()\n",
            "\n",
            "    # Gerando um relatório sobre um tema\n",
            "    tema_exemplo = \"Impacto da IA na agricultura brasileira\"\n",
            "    relatorio_gerado = relatorio_generator.gerar_relatorio(tema_exemplo)\n",
            "    print(\"\\n--- Relatório Gerado ---\")\n",
            "    print(relatorio_gerado)\n",
            "\n",
            "    # Exemplo com um prompt personalizado e parâmetros de chat (se MangabaAgent suportar)\n",
            "    # relatorio_personalizado_generator = RelatorioIA(agent_id=\"relatorio_detalhado\")\n",
            "    # custom_prompt = \"Produza um relatório acadêmico detalhado sobre {data}, focado em inovações e desafios.\"\n",
            "    # relatorio_detalhado = relatorio_personalizado_generator.gerar_relatorio(\n",
            "    #     \"Desafios da sustentabilidade na cadeia de suprimentos\",\n",
            "    #     prompt_template=custom_prompt,\n",
            "    #     temperature=0.7, # Exemplo de parâmetro para o chat do agente\n",
            "    #     top_p=0.9\n",
            "    # )\n",
            "    # print(\"\\n--- Relatório Detalhado Personalizado ---\")\n",
            "    # print(relatorio_detalhado)\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código é um bom ponto de partida para integrar uma busca e geração de relatório com IA. No entanto, há várias melhorias que podem ser aplicadas seguindo boas práticas Python, especialmente em termos de robustez, flexibilidade, legibilidade e manutenibilidade.\n",
        "\n",
        "Aqui estão as sugestões, categorizadas para facilitar a revisão:\n",
        "\n",
        "---\n",
        "\n",
        "### **Revisão e Sugestões de Melhoria**\n",
        "\n",
        "#### 1. **Docstrings e Type Hinting**\n",
        "\n",
        "*   **Problema:** O código não possui docstrings nem type hints.\n",
        "*   **Melhoria:** Adicionar docstrings para a classe e seus métodos, explicando o propósito, parâmetros e retornos. Adicionar type hints para os argumentos dos métodos e retornos para melhorar a legibilidade e a detecção de erros.\n",
        "\n",
        "```python\n",
        "# ...\n",
        "class RelatorioIA:\n",
        "    \"\"\"\n",
        "    Classe para integrar busca, análise e geração de relatórios estruturados\n",
        "    utilizando um agente de IA (MangabaAgent).\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
        "        \"\"\"\n",
        "        Inicializa a classe RelatorioIA com um MangabaAgent configurado.\n",
        "\n",
        "        Args:\n",
        "            agent_id (str): ID único para o agente de IA.\n",
        "            model (str): Nome do modelo de IA a ser utilizado (e.g., \"gemini-2.5-flash\").\n",
        "            tools (list[str]): Lista de ferramentas a serem disponibilizadas para o agente.\n",
        "                                Padrão para [\"search\"] se não for fornecido.\n",
        "        \"\"\"\n",
        "        if tools is None:\n",
        "            tools = [\"search\"]\n",
        "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
        "\n",
        "    def gerar_relatorio(self, tema: str) -> str:\n",
        "        \"\"\"\n",
        "        Gera um relatório estruturado sobre um tema específico.\n",
        "\n",
        "        Realiza uma busca pelo tema, analisa os resultados e gera um relatório\n",
        "        crítico utilizando o agente de IA.\n",
        "\n",
        "        Args:\n",
        "            tema (str): O tópico ou tema sobre o qual o relatório será gerado.\n",
        "\n",
        "        Returns:\n",
        "            str: O relatório estruturado gerado pelo agente de IA.\n",
        "        \"\"\"\n",
        "        # ... (restante do método)\n",
        "```\n",
        "\n",
        "#### 2. **Flexibilidade e Configuração**\n",
        "\n",
        "*   **Problema:** O modelo de IA (`\"gemini-2.5-flash\"`) e as ferramentas (`[\"search\"]`) estão \"hardcoded\" no `__init__`.\n",
        "*   **Melhoria:** Permitir que o modelo e as ferramentas sejam configuráveis via parâmetros do construtor, com valores padrão razoáveis. Isso torna a classe mais reutilizável sem modificações diretas.\n",
        "\n",
        "```python\n",
        "# ... (ver exemplo acima no __init__)\n",
        "class RelatorioIA:\n",
        "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
        "        if tools is None:\n",
        "            tools = [\"search\"] # Garantir que o valor padrão seja um objeto mutável novo\n",
        "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
        "```\n",
        "\n",
        "#### 3. **Tratamento de Erros e Robustez**\n",
        "\n",
        "*   **Problema:** O código não lida com possíveis falhas na chamada `run_tool` ou `chat` (ex: erros de rede, API, resultados vazios).\n",
        "*   **Melhoria:**\n",
        "    *   Usar blocos `try-except` para capturar exceções das chamadas à API.\n",
        "    *   Verificar se os `resultados` da busca não estão vazios ou são inválidos antes de passá-los para a análise.\n",
        "    *   Retornar uma mensagem de erro ou lançar uma exceção personalizada em caso de falha.\n",
        "    *   Implementar logging para depuração e monitoramento.\n",
        "\n",
        "```python\n",
        "import logging\n",
        "# from mangaba_ai import MangabaAgent # Assumindo que MangabaAgent lança exceções específicas ou genéricas\n",
        "\n",
        "# Configuração básica de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class RelatorioIA:\n",
        "    # ... (init conforme revisado)\n",
        "\n",
        "    def gerar_relatorio(self, tema: str) -> str:\n",
        "        logging.info(f\"Iniciando geração de relatório para o tema: '{tema}'\")\n",
        "        resultados = None\n",
        "        try:\n",
        "            logging.info(f\"Executando busca por: '{tema}'\")\n",
        "            resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
        "            if not resultados:\n",
        "                logging.warning(f\"A busca por '{tema}' não retornou resultados.\")\n",
        "                return \"Não foi possível encontrar dados relevantes para gerar o relatório.\"\n",
        "            logging.info(f\"Resultados da busca obtidos. Tamanho: {len(str(resultados))} caracteres.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao executar a ferramenta 'search' para o tema '{tema}': {e}\")\n",
        "            return f\"Erro ao buscar dados: {e}\"\n",
        "\n",
        "        analise = None\n",
        "        try:\n",
        "            prompt = f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\"\n",
        "            logging.info(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
        "            # Considerar truncar 'resultados' se for muito longo para evitar limites de token\n",
        "            # ou dividir a tarefa para o agente.\n",
        "            analise = self.agent.chat(prompt)\n",
        "            if not analise:\n",
        "                logging.warning(\"O agente de IA não conseguiu gerar uma análise para os dados fornecidos.\")\n",
        "                return \"O agente de IA não conseguiu gerar um relatório.\"\n",
        "            logging.info(\"Relatório gerado com sucesso.\")\n",
        "            return analise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao gerar a análise/relatório pelo agente de IA para o tema '{tema}': {e}\")\n",
        "            return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
        "\n",
        "```\n",
        "\n",
        "#### 4. **Logging**\n",
        "\n",
        "*   **Problema:** O código não possui nenhum mecanismo de logging.\n",
        "*   **Melhoria:** Integrar a biblioteca `logging` do Python para registrar o fluxo de execução, avisos e erros. Isso é fundamental para depuração e monitoramento em produção. (Já incluído no exemplo de Tratamento de Erros).\n",
        "\n",
        "#### 5. **Separação de Preocupações e Reusabilidade (Prompt Engineering)**\n",
        "\n",
        "*   **Problema:** O prompt para o agente está \"hardcoded\" dentro do método `gerar_relatorio`.\n",
        "*   **Melhoria:**\n",
        "    *   Definir o prompt como uma constante da classe ou um parâmetro, permitindo ajustes sem modificar a lógica do método.\n",
        "    *   Considerar que prompts podem se tornar complexos. Uma abordagem mais estruturada (e.g., templates de prompt, separação de instruções e dados) pode ser útil.\n",
        "    *   Adicionar parâmetros ao `chat` como `temperature`, `top_p`, etc., se o `MangabaAgent` os suportar, para controlar a criatividade e a diversidade das respostas.\n",
        "\n",
        "```python\n",
        "# ...\n",
        "class RelatorioIA:\n",
        "    DEFAULT_REPORT_PROMPT = \"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{data}\"\n",
        "    # ... (init)\n",
        "\n",
        "    def gerar_relatorio(self, tema: str, prompt_template: str = None, **chat_kwargs) -> str:\n",
        "        # ... (código existente)\n",
        "\n",
        "        if prompt_template is None:\n",
        "            prompt_template = self.DEFAULT_REPORT_PROMPT\n",
        "\n",
        "        # Adicione lógica para lidar com a substituição de {data} ou outro placeholder\n",
        "        # Aqui, {data} será substituído por resultados\n",
        "        final_prompt = prompt_template.format(data=resultados)\n",
        "\n",
        "        try:\n",
        "            logging.info(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
        "            # **chat_kwargs permitiria passar temperatura, top_p, etc., se suportado pelo MangabaAgent.chat()\n",
        "            analise = self.agent.chat(final_prompt, **chat_kwargs)\n",
        "            # ... (restante do código)\n",
        "```\n",
        "\n",
        "#### 6. **Gerenciamento de Tamanho de Entrada (Token Limits)**\n",
        "\n",
        "*   **Problema:** Os resultados da busca (`resultados`) podem ser muito longos, excedendo os limites de token do modelo de IA e/ou aumentando os custos.\n",
        "*   **Melhoria:** Implementar uma estratégia para gerenciar o tamanho dos `resultados` antes de passá-los para o `chat` do agente. Isso pode envolver:\n",
        "    *   **Truncagem:** Simplesmente cortar o texto após um certo número de caracteres/tokens.\n",
        "    *   **Resumo Prévio:** Usar o próprio agente (ou um agente menor/mais barato) para resumir os resultados da busca *antes* de enviá-los para a geração do relatório final.\n",
        "    *   **Processamento em Batches:** Dividir os resultados em partes e processá-las individualmente.\n",
        "\n",
        "```python\n",
        "# ... (dentro de gerar_relatorio, após obter resultados)\n",
        "MAX_RESULT_LENGTH = 8000  # Exemplo: Limite de caracteres para evitar estouro de tokens\n",
        "if len(str(resultados)) > MAX_RESULT_LENGTH:\n",
        "    logging.warning(f\"Resultados da busca excedem {MAX_RESULT_LENGTH} caracteres. Truncando.\")\n",
        "    # Uma forma simples de truncar (pode ser melhorado para truncar em limites de frase, etc.)\n",
        "    resultados_truncados = str(resultados)[:MAX_RESULT_LENGTH] + \" [TRUNCADO...]\"\n",
        "else:\n",
        "    resultados_truncados = resultados\n",
        "\n",
        "prompt = f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados_truncados}\"\n",
        "# ... (restante do método)\n",
        "```\n",
        "\n",
        "#### 7. **Considerações Adicionais (Async/Wait)**\n",
        "\n",
        "*   **Problema:** Se as chamadas à API `run_tool` e `chat` forem I/O-bound (o que é muito provável), o código atual é síncrono e bloqueará a execução.\n",
        "*   **Melhoria (futura):** Se `MangabaAgent` suportar chamadas assíncronas (e.g., `async_run_tool`, `async_chat`), considere refatorar a classe para usar `asyncio` e `await` para melhor desempenho em aplicações que precisam de concorrência.\n",
        "\n",
        "```python\n",
        "# Exemplo hipotético com async/await (requer MangabaAgent assíncrono)\n",
        "# class RelatorioIA:\n",
        "#     # ...\n",
        "#     async def gerar_relatorio_async(self, tema: str) -> str:\n",
        "#         try:\n",
        "#             resultados = await self.agent.async_run_tool(\"search\", parameters={\"query\": tema})\n",
        "#             if not resultados:\n",
        "#                 return \"Não foi possível encontrar dados relevantes.\"\n",
        "#         except Exception as e:\n",
        "#             return f\"Erro ao buscar dados: {e}\"\n",
        "#\n",
        "#         try:\n",
        "#             analise = await self.agent.async_chat(f\"Analise criticamente os dados abaixo e gere um relatório estruturado:\\n{resultados}\")\n",
        "#             return analise\n",
        "#         except Exception as e:\n",
        "#             return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Código Final Sugerido (com as melhorias principais)**\n",
        "\n",
        "```python\n",
        "import logging\n",
        "from mangaba_ai import MangabaAgent # Assumindo que esta biblioteca está instalada e funcional\n",
        "\n",
        "# Configuração básica de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class RelatorioIA:\n",
        "    \"\"\"\n",
        "    Classe para integrar busca, análise e geração de relatórios estruturados\n",
        "    utilizando um agente de IA (MangabaAgent).\n",
        "    \"\"\"\n",
        "    DEFAULT_REPORT_PROMPT_TEMPLATE = (\n",
        "        \"Com base nos dados fornecidos, analise criticamente as informações e gere um relatório estruturado.\\n\"\n",
        "        \"O relatório deve ser objetivo, cobrir os pontos principais e apresentar uma conclusão.\\n\"\n",
        "        \"Dados para análise:\\n{data}\"\n",
        "    )\n",
        "    MAX_SEARCH_RESULTS_TOKENS = 4000 # Exemplo: Limite de tokens para resultados de busca antes de enviar ao LLM\n",
        "\n",
        "    def __init__(self, agent_id: str = \"relatorio\", model: str = \"gemini-2.5-flash\", tools: list[str] = None):\n",
        "        \"\"\"\n",
        "        Inicializa a classe RelatorioIA com um MangabaAgent configurado.\n",
        "\n",
        "        Args:\n",
        "            agent_id (str): ID único para o agente de IA.\n",
        "            model (str): Nome do modelo de IA a ser utilizado (e.g., \"gemini-2.5-flash\").\n",
        "            tools (list[str]): Lista de ferramentas a serem disponibilizadas para o agente.\n",
        "                                Padrão para [\"search\"] se não for fornecido.\n",
        "        \"\"\"\n",
        "        if tools is None:\n",
        "            tools = [\"search\"] # Garantir que o valor padrão seja um objeto mutável novo\n",
        "        self.agent = MangabaAgent(model=model, agent_id=agent_id, tools=tools)\n",
        "        logging.info(f\"RelatorioIA inicializado com agent_id='{agent_id}', model='{model}', tools={tools}\")\n",
        "\n",
        "    def gerar_relatorio(self, tema: str, prompt_template: str = None, **chat_kwargs) -> str:\n",
        "        \"\"\"\n",
        "        Gera um relatório estruturado sobre um tema específico.\n",
        "\n",
        "        Realiza uma busca pelo tema, analisa os resultados e gera um relatório\n",
        "        crítico utilizando o agente de IA.\n",
        "\n",
        "        Args:\n",
        "            tema (str): O tópico ou tema sobre o qual o relatório será gerado.\n",
        "            prompt_template (str, opcional): Template de string para o prompt do agente.\n",
        "                                             Deve conter '{data}' para a inserção dos resultados.\n",
        "                                             Usa DEFAULT_REPORT_PROMPT_TEMPLATE se None.\n",
        "            **chat_kwargs: Argumentos adicionais a serem passados para o método `self.agent.chat()`,\n",
        "                           como `temperature`, `top_p`, etc., se suportados pelo MangabaAgent.\n",
        "\n",
        "        Returns:\n",
        "            str: O relatório estruturado gerado pelo agente de IA, ou uma mensagem de erro.\n",
        "        \"\"\"\n",
        "        logging.info(f\"Iniciando geração de relatório para o tema: '{tema}'\")\n",
        "        raw_resultados = None\n",
        "\n",
        "        # 1. Executar a busca\n",
        "        try:\n",
        "            logging.debug(f\"Executando busca por: '{tema}' com a ferramenta 'search'.\")\n",
        "            raw_resultados = self.agent.run_tool(\"search\", parameters={\"query\": tema})\n",
        "\n",
        "            if not raw_resultados:\n",
        "                logging.warning(f\"A busca por '{tema}' não retornou resultados. Não é possível gerar o relatório.\")\n",
        "                return \"Não foi possível encontrar dados relevantes para gerar o relatório.\"\n",
        "\n",
        "            logging.info(f\"Resultados da busca obtidos. Tamanho: {len(str(raw_resultados))} caracteres.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao executar a ferramenta 'search' para o tema '{tema}': {e}\", exc_info=True)\n",
        "            return f\"Erro ao buscar dados: {e}\"\n",
        "\n",
        "        # 2. Pré-processar resultados (gerenciar limites de tokens)\n",
        "        processed_resultados = str(raw_resultados)\n",
        "        # Assumindo uma função auxiliar para estimar tokens ou truncar\n",
        "        # Em um cenário real, você usaria um tokenizer específico do modelo\n",
        "        if len(processed_resultados.split()) > self.MAX_SEARCH_RESULTS_TOKENS:\n",
        "            logging.warning(f\"Resultados da busca excedem {self.MAX_SEARCH_RESULTS_TOKENS} tokens (aproximadamente). Truncando.\")\n",
        "            # Truncagem simples por palavras, pode ser refinado\n",
        "            processed_resultados = \" \".join(processed_resultados.split()[:self.MAX_SEARCH_RESULTS_TOKENS]) + \"\\n[...Dados truncados para caber no limite do modelo...]\"\n",
        "            logging.debug(f\"Resultados truncados. Novo tamanho: {len(processed_resultados)} caracteres.\")\n",
        "\n",
        "        # 3. Gerar o relatório com o agente de IA\n",
        "        analise = None\n",
        "        try:\n",
        "            current_prompt_template = prompt_template if prompt_template is not None else self.DEFAULT_REPORT_PROMPT_TEMPLATE\n",
        "            final_prompt = current_prompt_template.format(data=processed_resultados)\n",
        "\n",
        "            logging.debug(\"Iniciando análise e geração do relatório com o agente de IA.\")\n",
        "            analise = self.agent.chat(final_prompt, **chat_kwargs)\n",
        "\n",
        "            if not analise:\n",
        "                logging.warning(\"O agente de IA não conseguiu gerar uma análise/relatório significativo para os dados fornecidos.\")\n",
        "                return \"O agente de IA não conseguiu gerar um relatório com os dados disponíveis.\"\n",
        "\n",
        "            logging.info(\"Relatório gerado com sucesso.\")\n",
        "            return analise\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao gerar a análise/relatório pelo agente de IA para o tema '{tema}': {e}\", exc_info=True)\n",
        "            return f\"Erro ao analisar dados e gerar relatório: {e}\"\n",
        "\n",
        "# --- Exemplo de Uso ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Instanciando a classe com configurações padrão\n",
        "    relatorio_generator = RelatorioIA()\n",
        "\n",
        "    # Gerando um relatório sobre um tema\n",
        "    tema_exemplo = \"Impacto da IA na agricultura brasileira\"\n",
        "    relatorio_gerado = relatorio_generator.gerar_relatorio(tema_exemplo)\n",
        "    print(\"\\n--- Relatório Gerado ---\")\n",
        "    print(relatorio_gerado)\n",
        "\n",
        "    # Exemplo com um prompt personalizado e parâmetros de chat (se MangabaAgent suportar)\n",
        "    # relatorio_personalizado_generator = RelatorioIA(agent_id=\"relatorio_detalhado\")\n",
        "    # custom_prompt = \"Produza um relatório acadêmico detalhado sobre {data}, focado em inovações e desafios.\"\n",
        "    # relatorio_detalhado = relatorio_personalizado_generator.gerar_relatorio(\n",
        "    #     \"Desafios da sustentabilidade na cadeia de suprimentos\",\n",
        "    #     prompt_template=custom_prompt,\n",
        "    #     temperature=0.7, # Exemplo de parâmetro para o chat do agente\n",
        "    #     top_p=0.9\n",
        "    # )\n",
        "    # print(\"\\n--- Relatório Detalhado Personalizado ---\")\n",
        "    # print(relatorio_detalhado)\n",
        "```"
      ],
      "metadata": {
        "id": "VTxSAHhBrxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Classe que interage com múltiplas ferramentas e combina resultados\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class MultiFerramentas:\n",
        "    def __init__(self, agent_id=\"multitools\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id, tools=[\"calculator\", \"search\"])\n",
        "\n",
        "    def pesquisa_com_calculo(self, assunto, expressao):\n",
        "        resultado_pesquisa = self.agent.run_tool(\"search\", parameters={\"query\": assunto})\n",
        "        resultado_calculo = self.agent.run_tool(\"calculator\", parameters={\"expression\": expressao})\n",
        "        analise = self.agent.chat(\n",
        "            f\"Com base na pesquisa:\\n{resultado_pesquisa}\\nE no resultado do cálculo ({expressao} = {resultado_calculo}), gere uma conclusão prática.\"\n",
        "        )\n",
        "        return analise\n",
        "\n",
        "multi = MultiFerramentas()\n",
        "print(multi.pesquisa_com_calculo(\"produção de energia solar no Brasil\", \"365*5.5\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "aXPOp6BXuGGW",
        "outputId": "e87bebef-c7a3-46a8-d022-086d20626f34"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MangabaAgent.__init__() got an unexpected keyword argument 'tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1650022045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manalise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmulti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiFerramentas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpesquisa_com_calculo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"produção de energia solar no Brasil\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"365*5.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1650022045.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_id)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiFerramentas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multitools\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMangabaAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.5-flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"calculator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"search\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpesquisa_com_calculo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massunto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpressao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MangabaAgent.__init__() got an unexpected keyword argument 'tools'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Classe que realiza entrevista técnica simulada com base em tema\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class EntrevistadorIA:\n",
        "    def __init__(self, agent_id=\"entrevistador\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def simular_entrevista(self, tema, respostas_usuario):\n",
        "        perguntas = self.agent.chat(f\"Crie 3 perguntas técnicas sobre: {tema}\")\n",
        "        respostas = []\n",
        "        for pergunta, resposta_usuario in zip(perguntas.split('\\n'), respostas_usuario):\n",
        "            resposta_ia = self.agent.chat(f\"{pergunta}\\nResposta do candidato: {resposta_usuario}\\nAvalie a resposta e dê feedback.\")\n",
        "            respostas.append(resposta_ia)\n",
        "        return '\\n'.join(respostas)\n",
        "\n",
        "entrevistador = EntrevistadorIA()\n",
        "respostas_usuario = [\n",
        "    \"Um banco NoSQL armazena dados sem esquema fixo.\",\n",
        "    \"É indicado para grandes volumes de dados não estruturados.\",\n",
        "    \"Exemplo: MongoDB.\"\n",
        "]\n",
        "print(entrevistador.simular_entrevista(\"Banco de Dados NoSQL\", respostas_usuario))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a5pV1JgKur40",
        "outputId": "5aab4c53-4eb5-4417-a416-99920d0aeb06"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m21:00:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[entrevistador]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: entrevistador, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m21:00:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[entrevistador]\u001b[0m | \u001b[1m💬 Chat: Crie 3 perguntas técnicas sobre: Banco de Dados No... → Com certeza! Aqui estão 3 perguntas técnicas sobre...\u001b[0m\n",
            "\u001b[32m21:00:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[entrevistador]\u001b[0m | \u001b[1m💬 Chat: Com certeza! Aqui estão 3 perguntas técnicas sobre... → A afirmação do candidato (\"Um banco NoSQL armazena...\u001b[0m\n",
            "\u001b[32m21:00:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[entrevistador]\u001b[0m | \u001b[1m💬 Chat: \n",
            "Resposta do candidato: É indicado para grandes vo... → A afirmação do candidato (\"É indicado para grandes...\u001b[0m\n",
            "\u001b[32m21:01:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[entrevistador]\u001b[0m | \u001b[1m💬 Chat: 1.  **Diferenças Fundamentais e Tipos:** Quais são... → A resposta do candidato: \"Exemplo: MongoDB.\"\n",
            "\n",
            "---\n",
            "...\u001b[0m\n",
            "A afirmação do candidato (\"Um banco NoSQL armazena dados sem esquema fixo.\") está **correta**, mas a avaliação da sua adequação depende do contexto da pergunta que ele deveria estar respondendo.\n",
            "\n",
            "**Feedback:**\n",
            "\n",
            "1.  **Correcção da Afirmação:** A afirmação em si está **correta**. Uma das características mais distintivas dos bancos de dados NoSQL é, de facto, a sua capacidade de armazenar dados sem um esquema fixo ou rígido, o que oferece grande flexibilidade.\n",
            "\n",
            "2.  **Contexto da Pergunta:** A sua instrução original para o candidato (implícita pelo seu prompt anterior) era para \"Crie 3 perguntas técnicas sobre: Banco de Dados NoSQL\". No entanto, o candidato forneceu uma *afirmação* sobre NoSQL, e não as 3 perguntas solicitadas.\n",
            "\n",
            "    *   Se a intenção era que o candidato *respondesse* a uma das três perguntas que você gerou (que foram: 1. Diferenças Fundamentais e Tipos; 2. Consistência e Escalabilidade - Teorema CAP; 3. Modelagem de Dados e Consultas), então esta resposta é **extremamente insuficiente**. Embora a flexibilidade de esquema seja um ponto que poderia ser abordado na Pergunta 1 ou 3, a resposta do candidato não aborda a profundidade ou os múltiplos aspectos que as suas perguntas exigiam.\n",
            "\n",
            "3.  **Profundidade da Resposta:** A resposta é **muito superficial** para uma pergunta técnica. Em um contexto de entrevista ou avaliação técnica, espera-se que um candidato vá além de uma definição básica.\n",
            "\n",
            "**Em resumo:**\n",
            "\n",
            "*   **A afirmação \"Um banco NoSQL armazena dados sem esquema fixo\" é tecnicamente correta.**\n",
            "*   **Como resposta às suas 3 perguntas técnicas geradas, é completamente inadequada e superficial.** Não aborda a complexidade, as nuances, os exemplos ou os trade-offs esperados em uma discussão técnica aprofundada sobre NoSQL.\n",
            "\n",
            "**Sugestão de Feedback para o Candidato:**\n",
            "\n",
            "\"Sua afirmação de que 'Um banco NoSQL armazena dados sem esquema fixo' está **correta** e capta uma das características mais importantes e distintivas dos bancos de dados NoSQL em comparação com os relacionais.\n",
            "\n",
            "No entanto, para o nível de profundidade esperado em uma pergunta técnica (e especialmente se a intenção era responder a uma das três perguntas que foram propostas, como as sobre categorias, Teorema CAP ou modelagem de dados), essa resposta é **muito genérica e superficial**.\n",
            "\n",
            "Em um contexto técnico, esperamos que você **elabore** sobre o que significa não ter um esquema fixo, quais são as **vantagens e desvantagens** dessa abordagem, para quais **cenários** ela é mais adequada e como isso **impacta** a forma como os dados são modelados e consultados. Idealmente, a resposta deveria ter abordado de forma mais completa e detalhada os pontos específicos levantados nas perguntas formuladas.\"\n",
            "A afirmação do candidato (\"É indicado para grandes volumes de dados não estruturados.\") está **correta**, mas novamente, a avaliação da sua adequação depende do contexto da pergunta que ele deveria estar respondendo e do nível de profundidade esperado.\n",
            "\n",
            "**Feedback:**\n",
            "\n",
            "1.  **Correcção da Afirmação:** A afirmação em si está **correta**. Uma das principais razões para a adoção de bancos de dados NoSQL é, de facto, a sua capacidade de gerenciar e armazenar grandes volumes de dados não estruturados ou semi-estruturados de forma eficiente, algo que bancos de dados relacionais tradicionais não fazem tão bem.\n",
            "\n",
            "2.  **Contexto da Pergunta:** As suas instruções originais para o candidato eram para \"Crie 3 perguntas técnicas sobre: Banco de Dados NoSQL\". As perguntas geradas foram:\n",
            "    *   1. Diferenças Fundamentais e Tipos\n",
            "    *   2. Consistência e Escalabilidade - Teorema CAP\n",
            "    *   3. Modelagem de Dados e Consultas\n",
            "\n",
            "    A resposta do candidato é uma característica geral dos bancos NoSQL, que poderia ser mencionada como um **cenário de uso** na Pergunta 1 ou como uma **consequência da flexibilidade de esquema** na Pergunta 3. No entanto, como resposta *independente* ou como tentativa de responder *qualquer uma* das suas perguntas com profundidade, ela é **extremamente insuficiente e genérica**. Não aborda os múltiplos aspectos, a complexidade ou as comparações que as suas perguntas exigiam.\n",
            "\n",
            "3.  **Profundidade da Resposta:** A resposta é **muito superficial** para uma pergunta técnica. Em um contexto de entrevista ou avaliação técnica, espera-se que um candidato vá além de uma definição básica e explique o *porquê*, o *como*, as *implicações* e os *exemplos*.\n",
            "\n",
            "**Em resumo:**\n",
            "\n",
            "*   **A afirmação \"É indicado para grandes volumes de dados não estruturados\" é tecnicamente correta.**\n",
            "*   **Como resposta às suas 3 perguntas técnicas geradas, é completamente inadequada e superficial.** Ela não demonstra a compreensão técnica aprofundada esperada.\n",
            "\n",
            "**Sugestão de Feedback para o Candidato:**\n",
            "\n",
            "\"Sua afirmação de que um banco NoSQL 'é indicado para grandes volumes de dados não estruturados' está **correta** e capta uma das motivações chave para a sua adoção. Essa é, de facto, uma característica distintiva e um cenário de uso fundamental para muitas soluções NoSQL.\\n\\nNo entanto, para o nível de profundidade esperado em uma pergunta técnica (e especialmente se a intenção era responder a uma das três perguntas propostas, como as sobre categorias, Teorema CAP ou modelagem de dados), essa resposta é **muito genérica e superficial**.\\n\\nEm um contexto técnico, esperamos que você **elabore** sobre o que isso significa na prática. Por exemplo, você poderia explicar:\n",
            "*   **Por que** os bancos NoSQL são mais adequados para dados não estruturados do que os bancos relacionais? (Mencionar flexibilidade de esquema, escalabilidade horizontal, etc.)\n",
            "*   Quais são as **vantagens e desafios** de lidar com grandes volumes de dados não estruturados em um NoSQL?\n",
            "*   Poderia dar **exemplos** de tipos de dados não estruturados (logs, dados de sensores, posts de redes sociais) e como diferentes tipos de NoSQL (documento, chave-valor, colunar) podem ser usados para eles.\n",
            "*   Como isso **impacta** a forma como os dados são armazenados, indexados e consultados em comparação com um ambiente relacional.\n",
            "\n",
            "Aprofundar nesses pontos demonstraria uma compreensão técnica muito mais completa.\"\n",
            "A resposta do candidato: \"Exemplo: MongoDB.\"\n",
            "\n",
            "---\n",
            "\n",
            "**Avaliação da Resposta:**\n",
            "\n",
            "1.  **Correcção da Afirmação:** A menção a \"MongoDB\" está **correta** como um exemplo de um banco de dados NoSQL. Especificamente, o MongoDB é um banco de dados NoSQL do tipo **orientado a documentos**.\n",
            "\n",
            "2.  **Contexto da Pergunta:** A pergunta era complexa e multifacetada, exigindo:\n",
            "    *   A identificação das **principais categorias** de bancos de dados NoSQL (Chave-Valor, Documento, Colunar, Grafo).\n",
            "    *   Os **cenários de uso típicos** para **cada uma** dessas categorias.\n",
            "    *   Uma **comparação** da adequação dessas categorias em relação aos **bancos de dados relacionais tradicionais**.\n",
            "\n",
            "    A resposta do candidato, \"Exemplo: MongoDB\", aborda apenas uma pequena parte de uma das categorias (o tipo \"Documento\" tem MongoDB como um exemplo), mas não descreve a categoria, seus cenários de uso ou comparações. Portanto, como resposta à pergunta formulada, é **completamente insuficiente e não demonstra a compreensão esperada** dos múltiplos aspectos solicitados.\n",
            "\n",
            "3.  **Profundidade da Resposta:** A resposta é **extremamente superficial**. Em um contexto técnico, espera-se que um candidato vá muito além de apenas citar um exemplo, elaborando sobre os conceitos, aplicações, vantagens e desvantagens de cada tipo de NoSQL.\n",
            "\n",
            "**Em resumo:**\n",
            "\n",
            "*   A menção a \"MongoDB\" é um exemplo válido de um banco NoSQL (especificamente, orientado a documentos).\n",
            "*   Porém, como resposta à pergunta 1, que exige uma descrição das categorias, seus cenários de uso e comparação com bancos relacionais, é **totalmente inadequada e não demonstra o conhecimento técnico aprofundado esperado**.\n",
            "\n",
            "---\n",
            "\n",
            "**Sugestão de Feedback para o Candidato:**\n",
            "\n",
            "\"Sua menção a 'MongoDB' está **correta** como um exemplo de banco de dados NoSQL. De facto, ele é um dos bancos de dados NoSQL mais populares e se encaixa na categoria de **bancos de dados orientados a documentos**.\"\n",
            "\n",
            "\"No entanto, a pergunta pedia uma discussão **muito mais abrangente** sobre as diferentes categorias de bancos de dados NoSQL. Esperava-se que você identificasse e descrevesse as **principais categorias** (como Chave-Valor, Documento, Colunar e Grafo), e para **cada categoria**, discutisse seus **cenários de uso típicos** e, crucialmente, fizesse uma **comparação** sobre quando cada uma delas é mais adequada *em relação aos bancos de dados relacionais tradicionais*.\"\n",
            "\n",
            "\"Para o nível de profundidade esperado em uma pergunta técnica, seria importante **elaborar** sobre cada tipo. Por exemplo, ao mencionar o MongoDB (Documento), você poderia ter explicado que ele é ideal para dados semi-estruturados, documentos flexíveis como JSON, catálogos de produtos, perfis de usuário, etc., destacando a flexibilidade de esquema como uma vantagem em comparação com a rigidez de um esquema relacional.\"\n",
            "\n",
            "\"Uma resposta completa demonstraria sua capacidade de diferenciar as tecnologias NoSQL, entender seus pontos fortes e fracos, e aplicá-los a problemas do mundo real, o que é essencial para um nível técnico.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "...\n",
        "A afirmação do candidato (\"Um banco NoSQL armazena dados sem esquema fixo.\") está **correta**, mas a avaliação da sua adequação depende do contexto da pergunta que ele deveria estar respondendo.\n",
        "\n",
        "**Feedback:**\n",
        "\n",
        "1.  **Correcção da Afirmação:** A afirmação em si está **correta**. Uma das características mais distintivas dos bancos de dados NoSQL é, de facto, a sua capacidade de armazenar dados sem um esquema fixo ou rígido, o que oferece grande flexibilidade.\n",
        "\n",
        "2.  **Contexto da Pergunta:** A sua instrução original para o candidato (implícita pelo seu prompt anterior) era para \"Crie 3 perguntas técnicas sobre: Banco de Dados NoSQL\". No entanto, o candidato forneceu uma *afirmação* sobre NoSQL, e não as 3 perguntas solicitadas.\n",
        "\n",
        "    *   Se a intenção era que o candidato *respondesse* a uma das três perguntas que você gerou (que foram: 1. Diferenças Fundamentais e Tipos; 2. Consistência e Escalabilidade - Teorema CAP; 3. Modelagem de Dados e Consultas), então esta resposta é **extremamente insuficiente**. Embora a flexibilidade de esquema seja um ponto que poderia ser abordado na Pergunta 1 ou 3, a resposta do candidato não aborda a profundidade ou os múltiplos aspectos que as suas perguntas exigiam.\n",
        "\n",
        "3.  **Profundidade da Resposta:** A resposta é **muito superficial** para uma pergunta técnica. Em um contexto de entrevista ou avaliação técnica, espera-se que um candidato vá além de uma definição básica.\n",
        "\n",
        "**Em resumo:**\n",
        "\n",
        "*   **A afirmação \"Um banco NoSQL armazena dados sem esquema fixo\" é tecnicamente correta.**\n",
        "*   **Como resposta às suas 3 perguntas técnicas geradas, é completamente inadequada e superficial.** Não aborda a complexidade, as nuances, os exemplos ou os trade-offs esperados em uma discussão técnica aprofundada sobre NoSQL.\n",
        "\n",
        "**Sugestão de Feedback para o Candidato:**\n",
        "\n",
        "\"Sua afirmação de que 'Um banco NoSQL armazena dados sem esquema fixo' está **correta** e capta uma das características mais importantes e distintivas dos bancos de dados NoSQL em comparação com os relacionais.\n",
        "\n",
        "No entanto, para o nível de profundidade esperado em uma pergunta técnica (e especialmente se a intenção era responder a uma das três perguntas que foram propostas, como as sobre categorias, Teorema CAP ou modelagem de dados), essa resposta é **muito genérica e superficial**.\n",
        "\n",
        "Em um contexto técnico, esperamos que você **elabore** sobre o que significa não ter um esquema fixo, quais são as **vantagens e desvantagens** dessa abordagem, para quais **cenários** ela é mais adequada e como isso **impacta** a forma como os dados são modelados e consultados. Idealmente, a resposta deveria ter abordado de forma mais completa e detalhada os pontos específicos levantados nas perguntas formuladas.\"\n",
        "A afirmação do candidato (\"É indicado para grandes volumes de dados não estruturados.\") está **correta**, mas novamente, a avaliação da sua adequação depende do contexto da pergunta que ele deveria estar respondendo e do nível de profundidade esperado.\n",
        "\n",
        "**Feedback:**\n",
        "\n",
        "1.  **Correcção da Afirmação:** A afirmação em si está **correta**. Uma das principais razões para a adoção de bancos de dados NoSQL é, de facto, a sua capacidade de gerenciar e armazenar grandes volumes de dados não estruturados ou semi-estruturados de forma eficiente, algo que bancos de dados relacionais tradicionais não fazem tão bem.\n",
        "\n",
        "2.  **Contexto da Pergunta:** As suas instruções originais para o candidato eram para \"Crie 3 perguntas técnicas sobre: Banco de Dados NoSQL\". As perguntas geradas foram:\n",
        "    *   1. Diferenças Fundamentais e Tipos\n",
        "    *   2. Consistência e Escalabilidade - Teorema CAP\n",
        "    *   3. Modelagem de Dados e Consultas\n",
        "\n",
        "    A resposta do candidato é uma característica geral dos bancos NoSQL, que poderia ser mencionada como um **cenário de uso** na Pergunta 1 ou como uma **consequência da flexibilidade de esquema** na Pergunta 3. No entanto, como resposta *independente* ou como tentativa de responder *qualquer uma* das suas perguntas com profundidade, ela é **extremamente insuficiente e genérica**. Não aborda os múltiplos aspectos, a complexidade ou as comparações que as suas perguntas exigiam.\n",
        "\n",
        "3.  **Profundidade da Resposta:** A resposta é **muito superficial** para uma pergunta técnica. Em um contexto de entrevista ou avaliação técnica, espera-se que um candidato vá além de uma definição básica e explique o *porquê*, o *como*, as *implicações* e os *exemplos*.\n",
        "\n",
        "**Em resumo:**\n",
        "\n",
        "*   **A afirmação \"É indicado para grandes volumes de dados não estruturados\" é tecnicamente correta.**\n",
        "*   **Como resposta às suas 3 perguntas técnicas geradas, é completamente inadequada e superficial.** Ela não demonstra a compreensão técnica aprofundada esperada.\n",
        "\n",
        "**Sugestão de Feedback para o Candidato:**\n",
        "\n",
        "\"Sua afirmação de que um banco NoSQL 'é indicado para grandes volumes de dados não estruturados' está **correta** e capta uma das motivações chave para a sua adoção. Essa é, de facto, uma característica distintiva e um cenário de uso fundamental para muitas soluções NoSQL.\\n\\nNo entanto, para o nível de profundidade esperado em uma pergunta técnica (e especialmente se a intenção era responder a uma das três perguntas propostas, como as sobre categorias, Teorema CAP ou modelagem de dados), essa resposta é **muito genérica e superficial**.\\n\\nEm um contexto técnico, esperamos que você **elabore** sobre o que isso significa na prática. Por exemplo, você poderia explicar:\n",
        "*   **Por que** os bancos NoSQL são mais adequados para dados não estruturados do que os bancos relacionais? (Mencionar flexibilidade de esquema, escalabilidade horizontal, etc.)\n",
        "*   Quais são as **vantagens e desafios** de lidar com grandes volumes de dados não estruturados em um NoSQL?\n",
        "*   Poderia dar **exemplos** de tipos de dados não estruturados (logs, dados de sensores, posts de redes sociais) e como diferentes tipos de NoSQL (documento, chave-valor, colunar) podem ser usados para eles.\n",
        "*   Como isso **impacta** a forma como os dados são armazenados, indexados e consultados em comparação com um ambiente relacional.\n",
        "\n",
        "Aprofundar nesses pontos demonstraria uma compreensão técnica muito mais completa.\"\n",
        "A resposta do candidato: \"Exemplo: MongoDB.\"\n",
        "\n",
        "---\n",
        "\n",
        "**Avaliação da Resposta:**\n",
        "\n",
        "1.  **Correcção da Afirmação:** A menção a \"MongoDB\" está **correta** como um exemplo de um banco de dados NoSQL. Especificamente, o MongoDB é um banco de dados NoSQL do tipo **orientado a documentos**.\n",
        "\n",
        "2.  **Contexto da Pergunta:** A pergunta era complexa e multifacetada, exigindo:\n",
        "    *   A identificação das **principais categorias** de bancos de dados NoSQL (Chave-Valor, Documento, Colunar, Grafo).\n",
        "    *   Os **cenários de uso típicos** para **cada uma** dessas categorias.\n",
        "    *   Uma **comparação** da adequação dessas categorias em relação aos **bancos de dados relacionais tradicionais**.\n",
        "\n",
        "    A resposta do candidato, \"Exemplo: MongoDB\", aborda apenas uma pequena parte de uma das categorias (o tipo \"Documento\" tem MongoDB como um exemplo), mas não descreve a categoria, seus cenários de uso ou comparações. Portanto, como resposta à pergunta formulada, é **completamente insuficiente e não demonstra a compreensão esperada** dos múltiplos aspectos solicitados.\n",
        "\n",
        "3.  **Profundidade da Resposta:** A resposta é **extremamente superficial**. Em um contexto técnico, espera-se que um candidato vá muito além de apenas citar um exemplo, elaborando sobre os conceitos, aplicações, vantagens e desvantagens de cada tipo de NoSQL.\n",
        "\n",
        "**Em resumo:**\n",
        "\n",
        "*   A menção a \"MongoDB\" é um exemplo válido de um banco NoSQL (especificamente, orientado a documentos).\n",
        "*   Porém, como resposta à pergunta 1, que exige uma descrição das categorias, seus cenários de uso e comparação com bancos relacionais, é **totalmente inadequada e não demonstra o conhecimento técnico aprofundado esperado**.\n",
        "\n",
        "---\n",
        "\n",
        "**Sugestão de Feedback para o Candidato:**\n",
        "\n",
        "\"Sua menção a 'MongoDB' está **correta** como um exemplo de banco de dados NoSQL. De facto, ele é um dos bancos de dados NoSQL mais populares e se encaixa na categoria de **bancos de dados orientados a documentos**.\"\n",
        "\n",
        "\"No entanto, a pergunta pedia uma discussão **muito mais abrangente** sobre as diferentes categorias de bancos de dados NoSQL. Esperava-se que você identificasse e descrevesse as **principais categorias** (como Chave-Valor, Documento, Colunar e Grafo), e para **cada categoria**, discutisse seus **cenários de uso típicos** e, crucialmente, fizesse uma **comparação** sobre quando cada uma delas é mais adequada *em relação aos bancos de dados relacionais tradicionais*.\"\n",
        "\n",
        "\"Para o nível de profundidade esperado em uma pergunta técnica, seria importante **elaborar** sobre cada tipo. Por exemplo, ao mencionar o MongoDB (Documento), você poderia ter explicado que ele é ideal para dados semi-estruturados, documentos flexíveis como JSON, catálogos de produtos, perfis de usuário, etc., destacando a flexibilidade de esquema como uma vantagem em comparação com a rigidez de um esquema relacional.\"\n",
        "\n",
        "\"Uma resposta completa demonstraria sua capacidade de diferenciar as tecnologias NoSQL, entender seus pontos fortes e fracos, e aplicá-los a problemas do mundo real, o que é essencial para um nível técnico.\""
      ],
      "metadata": {
        "id": "R2gF3-QXvMKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Classe para análise comparativa de tecnologias com relatório final\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class ComparadorTecnologico:\n",
        "    def __init__(self, agent_id=\"comparador\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def comparar(self, tecnologia1, tecnologia2):\n",
        "        prompt = f\"Compare de forma detalhada as tecnologias: {tecnologia1} e {tecnologia2}, citando vantagens, desvantagens e possíveis aplicações.\"\n",
        "        return self.agent.chat(prompt)\n",
        "\n",
        "comparador = ComparadorTecnologico()\n",
        "print(comparador.comparar(\"TensorFlow\", \"PyTorch\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wRcu3gl2vgI4",
        "outputId": "8c1f2e2c-2db0-4af1-bb9a-f81e728cc0ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m21:05:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[comparador]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: comparador, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m21:05:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[comparador]\u001b[0m | \u001b[1m💬 Chat: Compare de forma detalhada as tecnologias: TensorF... → Com certeza! TensorFlow e PyTorch são as duas bibl...\u001b[0m\n",
            "Com certeza! TensorFlow e PyTorch são as duas bibliotecas de deep learning mais populares e poderosas do mercado. Ambas permitem construir, treinar e implantar modelos complexos, mas possuem filosofias de design e pontos fortes distintos.\n",
            "\n",
            "Vamos compará-las detalhadamente:\n",
            "\n",
            "---\n",
            "\n",
            "## **TensorFlow**\n",
            "\n",
            "**Origem:** Desenvolvido e mantido pelo Google.\n",
            "**Lançamento:** 2015\n",
            "**Versão Atual Relevante:** TensorFlow 2.x (que incorporou Keras e adotou execução eager por padrão, aproximando-se do PyTorch em termos de usabilidade).\n",
            "\n",
            "### **Filosofia / Modelo de Execução (TensorFlow 2.x):**\n",
            "\n",
            "Historicamente, o TensorFlow era conhecido por sua **arquitetura de grafo estático**. Isso significava que você precisava definir todo o grafo computacional (todas as operações e como elas se conectam) *antes* de executar qualquer computação.\n",
            "Com o TensorFlow 2.x, a **execução eager (dinâmica)** se tornou o padrão. Isso permite que as operações sejam executadas imediatamente e os valores sejam retornados, tornando o desenvolvimento e a depuração muito mais intuitivos e semelhantes ao Python tradicional. No entanto, ele ainda oferece `tf.function` para compilar partes do código Python em grafos estáticos otimizados para desempenho e deployment.\n",
            "\n",
            "### **Vantagens do TensorFlow:**\n",
            "\n",
            "1.  **Ecossistema Abrangente e Maduro:** O TensorFlow possui um ecossistema gigantesco, incluindo:\n",
            "    *   **TensorFlow Serving:** Para deployment eficiente de modelos em produção.\n",
            "    *   **TensorFlow Lite:** Para modelos em dispositivos móveis e embarcados (edge devices).\n",
            "    *   **TensorFlow.js:** Para executar modelos no navegador ou em Node.js.\n",
            "    *   **TensorFlow Extended (TFX):** Uma plataforma completa para produção de ML (MLOps), abrangendo desde a ingestão de dados até o monitoramento do modelo.\n",
            "    *   **TensorBoard:** Uma poderosa ferramenta de visualização para monitorar métricas de treinamento, visualizar grafos e embeddings.\n",
            "2.  **Escalabilidade e Produção:** Historicamente, o TensorFlow era a escolha dominante para grandes implementações em escala de produção e sistemas distribuídos complexos. Seu design original de grafo estático permitia otimizações de compilação muito robustas. Embora o PyTorch tenha avançado muito aqui, o TF ainda é uma referência em MLOps e deployment.\n",
            "3.  **Suporte Corporativo Robusto:** Ser desenvolvido pelo Google garante um forte suporte corporativo, vastos recursos e integração nativa com tecnologias do Google Cloud (TPUs, Google Kubernetes Engine, etc.).\n",
            "4.  **Integração Keras:** Keras é uma API de alto nível extremamente popular e fácil de usar. No TF 2.x, Keras foi totalmente integrado como a API padrão para construção de modelos, tornando o TensorFlow muito mais acessível e produtivo para iniciantes e desenvolvedores que buscam agilidade.\n",
            "5.  **Otimização para Hardware Específico:** Forte suporte para TPUs (Tensor Processing Units) do Google, que podem oferecer aceleração significativa para certos tipos de cargas de trabalho.\n",
            "\n",
            "### **Desvantagens do TensorFlow:**\n",
            "\n",
            "1.  **Curva de Aprendizagem (Histórico):** Antes do TF 2.x, sua natureza de grafo estático tornava a depuração mais complexa e a API, mais verbosa e menos \"Pythonic\" para muitos. Embora tenha melhorado drasticamente, alguns ainda podem achá-lo mais complexo para controle de baixo nível em comparação com PyTorch.\n",
            "2.  **Menos \"Pythonic\" (para alguns casos):** Mesmo com o eager execution, alguns desenvolvedores acham que o fluxo de trabalho pode ser menos alinhado com as práticas de codificação Python idiomáticas quando comparado ao PyTorch, especialmente em cenários muito customizados.\n",
            "3.  **Sobrepeso para Projetos Pequenos:** Para protótipos rápidos ou projetos menores, a infraestrutura robusta e o ecossistema completo do TensorFlow podem parecer um pouco \"demais\", tornando o PyTorch uma opção mais leve e rápida para começar.\n",
            "\n",
            "### **Aplicações do TensorFlow:**\n",
            "\n",
            "*   **Sistemas de Recomendação em Escala:** Utilizado por grandes empresas para personalizar experiências de usuário.\n",
            "*   **Processamento de Linguagem Natural (NLP):** Treinamento de modelos de linguagem como BERT, T5 e outros grandes modelos para chatbots, tradução, análise de sentimento.\n",
            "*   **Visão Computacional:** Detecção de objetos, reconhecimento facial, segmentação de imagens em larga escala (por exemplo, em carros autônomos ou monitoramento de segurança).\n",
            "*   **Previsão de Séries Temporais:** Modelagem de dados financeiros, previsão de demanda, análise de sensores.\n",
            "*   **Desenvolvimento de Modelos para Edge Devices:** Com TensorFlow Lite, é a escolha ideal para integrar IA em smartphones, IoT e outros dispositivos com recursos limitados.\n",
            "*   **Grandes Implantações em Nuvem:** Se beneficiando da integração com GCP e TPUs.\n",
            "\n",
            "---\n",
            "\n",
            "## **PyTorch**\n",
            "\n",
            "**Origem:** Desenvolvido e mantido pelo Facebook (agora Meta).\n",
            "**Lançamento:** 2016\n",
            "\n",
            "### **Filosofia / Modelo de Execução:**\n",
            "\n",
            "PyTorch é construído em torno de uma **arquitetura de grafo dinâmico**, também conhecida como **execução eager por padrão**. Isso significa que o grafo computacional é construído em tempo de execução, à medida que cada operação é executada. É semelhante à forma como o Python padrão funciona, tornando-o muito intuitivo para os desenvolvedores Python. A biblioteca `autograd` do PyTorch é a espinha dorsal para a diferenciação automática.\n",
            "\n",
            "### **Vantagens do PyTorch:**\n",
            "\n",
            "1.  **Pythonic e Intuitivo:** Sua API é extremamente \"Pythonic\", o que significa que o código se parece e se comporta como código Python normal. Isso facilita o aprendizado para desenvolvedores Python e torna o prototipagem e a depuração mais rápidos.\n",
            "2.  **Grafo de Computação Dinâmico:** A principal vantagem. Permite flexibilidade incomparável, o que é crucial para modelos de pesquisa com estruturas de dados variáveis (como NLP com sequências de comprimento variável) ou modelos que precisam de lógica condicional que muda durante a execução.\n",
            "3.  **Facilidade de Depuração:** Graças ao grafo dinâmico, você pode usar depuradores Python padrão (como `pdb`) para inspecionar o código a qualquer momento, o que não era trivial com os grafos estáticos do TensorFlow.\n",
            "4.  **Prototipagem Rápida e Pesquisa:** Devido à sua flexibilidade e facilidade de uso, PyTorch se tornou a ferramenta preferida na comunidade de pesquisa e acadêmica. Muitos dos artigos de ponta em ML implementam seus modelos inicialmente em PyTorch.\n",
            "5.  **Comunidade Ativa e Crescente:** A comunidade de PyTorch é extremamente vibrante, especialmente em pesquisa. Há muitos tutoriais, exemplos e implementações de artigos disponíveis.\n",
            "6.  **TorchScript:** Permite a transição de modelos do modo eager para um modo de grafo estático otimizado para produção, preenchendo a lacuna que existia em relação ao TensorFlow para deployment.\n",
            "7.  **`torch.compile` (futuro/recente):** Uma nova funcionalidade para otimizar e acelerar ainda mais o treinamento e a inferência, compilando o código PyTorch para operações mais eficientes.\n",
            "\n",
            "### **Desvantagens do PyTorch:**\n",
            "\n",
            "1.  **Maturidade do Ecossistema de Produção (Histórico):** Embora tenha melhorado drasticamente com TorchScript, PyTorch Mobile e bibliotecas como `accelerate` da Hugging Face, ele historicamente tinha menos ferramentas robustas para deployment em larga escala e MLOps em comparação com o ecossystema TFX do TensorFlow. Essa lacuna está diminuindo rapidamente.\n",
            "2.  **Menos Ferramentas \"Out-of-the-box\" para Deployment:** Embora TorchScript ajude, o TensorFlow ainda possui uma vantagem em ferramentas como TensorFlow Serving, TensorFlow Lite, TensorFlow.js, que são mais maduras e integradas para cenários específicos.\n",
            "3.  **Visualização:** Embora você possa usar TensorBoard com PyTorch (via `torch.utils.tensorboard`), as ferramentas nativas de visualização podem não ser tão completas ou integradas quanto as do TensorFlow para certos aspectos.\n",
            "\n",
            "### **Aplicações do PyTorch:**\n",
            "\n",
            "*   **Pesquisa e Desenvolvimento:** Ideal para experimentar novas arquiteturas de rede e algoritmos de aprendizado de máquina.\n",
            "*   **Processamento de Linguagem Natural (NLP):** Particularmente eficaz para modelos com estruturas dinâmicas, como muitos modelos Transformer e seq2seq, e para bibliotecas como Hugging Face Transformers.\n",
            "*   **Visão Computacional:** Modelos para classificação de imagens, detecção de objetos e segmentação, especialmente em pesquisa e customização.\n",
            "*   **Reinforcement Learning:** A flexibilidade do grafo dinâmico é muito útil para algoritmos de RL que frequentemente envolvem lógica complexa e que muda dinamicamente.\n",
            "*   **Geração de Conteúdo:** Modelos generativos como GANs e VAEs, onde a flexibilidade do grafo é benéfica.\n",
            "*   **Projetos Acadêmicos e Startups:** Onde a agilidade no desenvolvimento e a capacidade de experimentar rapidamente são cruciais.\n",
            "\n",
            "---\n",
            "\n",
            "## **Conclusão e Quando Escolher Qual:**\n",
            "\n",
            "Tanto TensorFlow quanto PyTorch são ferramentas fantásticas e extremamente poderosas para deep learning. A rivalidade inicial de \"grafo estático vs. grafo dinâmico\" diminuiu consideravelmente com o TensorFlow 2.x adotando o eager execution e o PyTorch introduzindo TorchScript para otimização em produção. Há uma **convergência significativa** entre as duas.\n",
            "\n",
            "*   **Escolha TensorFlow se:**\n",
            "    *   Você precisa de uma solução **robusta e comprovada para deployment em larga escala e produção (MLOps)**, especialmente se estiver construindo um produto que será executado em diversas plataformas (web, mobile, edge).\n",
            "    *   Sua equipe já tem experiência com o ecossistema Google Cloud e TPUs.\n",
            "    *   Você aprecia uma **API de alto nível mais \"opiniosa\" (Keras)** para construir modelos rapidamente.\n",
            "    *   Você está trabalhando em um ambiente corporativo grande que valoriza um ecossistema completo e suporte empresarial.\n",
            "\n",
            "*   **Escolha PyTorch se:**\n",
            "    *   Você prioriza a **facilidade de uso, prototipagem rápida e depuração intuitiva**, especialmente se você vem de um background de desenvolvimento Python.\n",
            "    *   Você está envolvido em **pesquisa e desenvolvimento** de ponta, onde a flexibilidade para experimentar novas arquiteturas é crucial.\n",
            "    *   Sua equipe valoriza uma experiência de codificação mais **\"Pythonic\"** e transparente.\n",
            "    *   Você trabalha com modelos que exigem **lógica de grafo altamente dinâmica ou estruturas de dados variáveis**.\n",
            "\n",
            "Muitas empresas e pesquisadores acabam usando ambos, escolhendo a ferramenta mais adequada para cada fase ou aspecto do projeto. A melhor escolha, no final das contas, depende dos requisitos específicos do seu projeto, da experiência da sua equipe e das suas preferências pessoais.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com certeza! TensorFlow e PyTorch são as duas bibliotecas de deep learning mais populares e poderosas do mercado. Ambas permitem construir, treinar e implantar modelos complexos, mas possuem filosofias de design e pontos fortes distintos.\n",
        "\n",
        "Vamos compará-las detalhadamente:\n",
        "\n",
        "---\n",
        "\n",
        "## **TensorFlow**\n",
        "\n",
        "**Origem:** Desenvolvido e mantido pelo Google.\n",
        "**Lançamento:** 2015\n",
        "**Versão Atual Relevante:** TensorFlow 2.x (que incorporou Keras e adotou execução eager por padrão, aproximando-se do PyTorch em termos de usabilidade).\n",
        "\n",
        "### **Filosofia / Modelo de Execução (TensorFlow 2.x):**\n",
        "\n",
        "Historicamente, o TensorFlow era conhecido por sua **arquitetura de grafo estático**. Isso significava que você precisava definir todo o grafo computacional (todas as operações e como elas se conectam) *antes* de executar qualquer computação.\n",
        "Com o TensorFlow 2.x, a **execução eager (dinâmica)** se tornou o padrão. Isso permite que as operações sejam executadas imediatamente e os valores sejam retornados, tornando o desenvolvimento e a depuração muito mais intuitivos e semelhantes ao Python tradicional. No entanto, ele ainda oferece `tf.function` para compilar partes do código Python em grafos estáticos otimizados para desempenho e deployment.\n",
        "\n",
        "### **Vantagens do TensorFlow:**\n",
        "\n",
        "1.  **Ecossistema Abrangente e Maduro:** O TensorFlow possui um ecossistema gigantesco, incluindo:\n",
        "    *   **TensorFlow Serving:** Para deployment eficiente de modelos em produção.\n",
        "    *   **TensorFlow Lite:** Para modelos em dispositivos móveis e embarcados (edge devices).\n",
        "    *   **TensorFlow.js:** Para executar modelos no navegador ou em Node.js.\n",
        "    *   **TensorFlow Extended (TFX):** Uma plataforma completa para produção de ML (MLOps), abrangendo desde a ingestão de dados até o monitoramento do modelo.\n",
        "    *   **TensorBoard:** Uma poderosa ferramenta de visualização para monitorar métricas de treinamento, visualizar grafos e embeddings.\n",
        "2.  **Escalabilidade e Produção:** Historicamente, o TensorFlow era a escolha dominante para grandes implementações em escala de produção e sistemas distribuídos complexos. Seu design original de grafo estático permitia otimizações de compilação muito robustas. Embora o PyTorch tenha avançado muito aqui, o TF ainda é uma referência em MLOps e deployment.\n",
        "3.  **Suporte Corporativo Robusto:** Ser desenvolvido pelo Google garante um forte suporte corporativo, vastos recursos e integração nativa com tecnologias do Google Cloud (TPUs, Google Kubernetes Engine, etc.).\n",
        "4.  **Integração Keras:** Keras é uma API de alto nível extremamente popular e fácil de usar. No TF 2.x, Keras foi totalmente integrado como a API padrão para construção de modelos, tornando o TensorFlow muito mais acessível e produtivo para iniciantes e desenvolvedores que buscam agilidade.\n",
        "5.  **Otimização para Hardware Específico:** Forte suporte para TPUs (Tensor Processing Units) do Google, que podem oferecer aceleração significativa para certos tipos de cargas de trabalho.\n",
        "\n",
        "### **Desvantagens do TensorFlow:**\n",
        "\n",
        "1.  **Curva de Aprendizagem (Histórico):** Antes do TF 2.x, sua natureza de grafo estático tornava a depuração mais complexa e a API, mais verbosa e menos \"Pythonic\" para muitos. Embora tenha melhorado drasticamente, alguns ainda podem achá-lo mais complexo para controle de baixo nível em comparação com PyTorch.\n",
        "2.  **Menos \"Pythonic\" (para alguns casos):** Mesmo com o eager execution, alguns desenvolvedores acham que o fluxo de trabalho pode ser menos alinhado com as práticas de codificação Python idiomáticas quando comparado ao PyTorch, especialmente em cenários muito customizados.\n",
        "3.  **Sobrepeso para Projetos Pequenos:** Para protótipos rápidos ou projetos menores, a infraestrutura robusta e o ecossistema completo do TensorFlow podem parecer um pouco \"demais\", tornando o PyTorch uma opção mais leve e rápida para começar.\n",
        "\n",
        "### **Aplicações do TensorFlow:**\n",
        "\n",
        "*   **Sistemas de Recomendação em Escala:** Utilizado por grandes empresas para personalizar experiências de usuário.\n",
        "*   **Processamento de Linguagem Natural (NLP):** Treinamento de modelos de linguagem como BERT, T5 e outros grandes modelos para chatbots, tradução, análise de sentimento.\n",
        "*   **Visão Computacional:** Detecção de objetos, reconhecimento facial, segmentação de imagens em larga escala (por exemplo, em carros autônomos ou monitoramento de segurança).\n",
        "*   **Previsão de Séries Temporais:** Modelagem de dados financeiros, previsão de demanda, análise de sensores.\n",
        "*   **Desenvolvimento de Modelos para Edge Devices:** Com TensorFlow Lite, é a escolha ideal para integrar IA em smartphones, IoT e outros dispositivos com recursos limitados.\n",
        "*   **Grandes Implantações em Nuvem:** Se beneficiando da integração com GCP e TPUs.\n",
        "\n",
        "---\n",
        "\n",
        "## **PyTorch**\n",
        "\n",
        "**Origem:** Desenvolvido e mantido pelo Facebook (agora Meta).\n",
        "**Lançamento:** 2016\n",
        "\n",
        "### **Filosofia / Modelo de Execução:**\n",
        "\n",
        "PyTorch é construído em torno de uma **arquitetura de grafo dinâmico**, também conhecida como **execução eager por padrão**. Isso significa que o grafo computacional é construído em tempo de execução, à medida que cada operação é executada. É semelhante à forma como o Python padrão funciona, tornando-o muito intuitivo para os desenvolvedores Python. A biblioteca `autograd` do PyTorch é a espinha dorsal para a diferenciação automática.\n",
        "\n",
        "### **Vantagens do PyTorch:**\n",
        "\n",
        "1.  **Pythonic e Intuitivo:** Sua API é extremamente \"Pythonic\", o que significa que o código se parece e se comporta como código Python normal. Isso facilita o aprendizado para desenvolvedores Python e torna o prototipagem e a depuração mais rápidos.\n",
        "2.  **Grafo de Computação Dinâmico:** A principal vantagem. Permite flexibilidade incomparável, o que é crucial para modelos de pesquisa com estruturas de dados variáveis (como NLP com sequências de comprimento variável) ou modelos que precisam de lógica condicional que muda durante a execução.\n",
        "3.  **Facilidade de Depuração:** Graças ao grafo dinâmico, você pode usar depuradores Python padrão (como `pdb`) para inspecionar o código a qualquer momento, o que não era trivial com os grafos estáticos do TensorFlow.\n",
        "4.  **Prototipagem Rápida e Pesquisa:** Devido à sua flexibilidade e facilidade de uso, PyTorch se tornou a ferramenta preferida na comunidade de pesquisa e acadêmica. Muitos dos artigos de ponta em ML implementam seus modelos inicialmente em PyTorch.\n",
        "5.  **Comunidade Ativa e Crescente:** A comunidade de PyTorch é extremamente vibrante, especialmente em pesquisa. Há muitos tutoriais, exemplos e implementações de artigos disponíveis.\n",
        "6.  **TorchScript:** Permite a transição de modelos do modo eager para um modo de grafo estático otimizado para produção, preenchendo a lacuna que existia em relação ao TensorFlow para deployment.\n",
        "7.  **`torch.compile` (futuro/recente):** Uma nova funcionalidade para otimizar e acelerar ainda mais o treinamento e a inferência, compilando o código PyTorch para operações mais eficientes.\n",
        "\n",
        "### **Desvantagens do PyTorch:**\n",
        "\n",
        "1.  **Maturidade do Ecossistema de Produção (Histórico):** Embora tenha melhorado drasticamente com TorchScript, PyTorch Mobile e bibliotecas como `accelerate` da Hugging Face, ele historicamente tinha menos ferramentas robustas para deployment em larga escala e MLOps em comparação com o ecossystema TFX do TensorFlow. Essa lacuna está diminuindo rapidamente.\n",
        "2.  **Menos Ferramentas \"Out-of-the-box\" para Deployment:** Embora TorchScript ajude, o TensorFlow ainda possui uma vantagem em ferramentas como TensorFlow Serving, TensorFlow Lite, TensorFlow.js, que são mais maduras e integradas para cenários específicos.\n",
        "3.  **Visualização:** Embora você possa usar TensorBoard com PyTorch (via `torch.utils.tensorboard`), as ferramentas nativas de visualização podem não ser tão completas ou integradas quanto as do TensorFlow para certos aspectos.\n",
        "\n",
        "### **Aplicações do PyTorch:**\n",
        "\n",
        "*   **Pesquisa e Desenvolvimento:** Ideal para experimentar novas arquiteturas de rede e algoritmos de aprendizado de máquina.\n",
        "*   **Processamento de Linguagem Natural (NLP):** Particularmente eficaz para modelos com estruturas dinâmicas, como muitos modelos Transformer e seq2seq, e para bibliotecas como Hugging Face Transformers.\n",
        "*   **Visão Computacional:** Modelos para classificação de imagens, detecção de objetos e segmentação, especialmente em pesquisa e customização.\n",
        "*   **Reinforcement Learning:** A flexibilidade do grafo dinâmico é muito útil para algoritmos de RL que frequentemente envolvem lógica complexa e que muda dinamicamente.\n",
        "*   **Geração de Conteúdo:** Modelos generativos como GANs e VAEs, onde a flexibilidade do grafo é benéfica.\n",
        "*   **Projetos Acadêmicos e Startups:** Onde a agilidade no desenvolvimento e a capacidade de experimentar rapidamente são cruciais.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusão e Quando Escolher Qual:**\n",
        "\n",
        "Tanto TensorFlow quanto PyTorch são ferramentas fantásticas e extremamente poderosas para deep learning. A rivalidade inicial de \"grafo estático vs. grafo dinâmico\" diminuiu consideravelmente com o TensorFlow 2.x adotando o eager execution e o PyTorch introduzindo TorchScript para otimização em produção. Há uma **convergência significativa** entre as duas.\n",
        "\n",
        "*   **Escolha TensorFlow se:**\n",
        "    *   Você precisa de uma solução **robusta e comprovada para deployment em larga escala e produção (MLOps)**, especialmente se estiver construindo um produto que será executado em diversas plataformas (web, mobile, edge).\n",
        "    *   Sua equipe já tem experiência com o ecossistema Google Cloud e TPUs.\n",
        "    *   Você aprecia uma **API de alto nível mais \"opiniosa\" (Keras)** para construir modelos rapidamente.\n",
        "    *   Você está trabalhando em um ambiente corporativo grande que valoriza um ecossistema completo e suporte empresarial.\n",
        "\n",
        "*   **Escolha PyTorch se:**\n",
        "    *   Você prioriza a **facilidade de uso, prototipagem rápida e depuração intuitiva**, especialmente se você vem de um background de desenvolvimento Python.\n",
        "    *   Você está envolvido em **pesquisa e desenvolvimento** de ponta, onde a flexibilidade para experimentar novas arquiteturas é crucial.\n",
        "    *   Sua equipe valoriza uma experiência de codificação mais **\"Pythonic\"** e transparente.\n",
        "    *   Você trabalha com modelos que exigem **lógica de grafo altamente dinâmica ou estruturas de dados variáveis**.\n",
        "\n",
        "Muitas empresas e pesquisadores acabam usando ambos, escolhendo a ferramenta mais adequada para cada fase ou aspecto do projeto. A melhor escolha, no final das contas, depende dos requisitos específicos do seu projeto, da experiência da sua equipe e das suas preferências pessoais."
      ],
      "metadata": {
        "id": "0O-cHQ56xteQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class AtendimentoCliente:\n",
        "    def __init__(self, agent_id=\"atendimento\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "        self.contexto = []\n",
        "\n",
        "    def registrar_interacao(self, mensagem):\n",
        "        self.contexto.append(mensagem)\n",
        "        resposta = self.agent.chat(f\"Contexto: {self.contexto}\\nUsuário: {mensagem}\")\n",
        "        self.contexto.append(resposta)\n",
        "        return resposta\n",
        "\n",
        "atendimento = AtendimentoCliente()\n",
        "print(atendimento.registrar_interacao(\"Meu pedido está atrasado.\"))\n",
        "print(atendimento.registrar_interacao(\"Quero saber o status da entrega.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "yQztWPLIx6XR",
        "outputId": "455f9f96-5ccf-4e13-b396-d5fdc93e3f32"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m21:17:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[atendimento]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: atendimento, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m21:17:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[atendimento]\u001b[0m | \u001b[1m💬 Chat: Contexto: ['Meu pedido está atrasado.']\n",
            "Usuário: M... → Lamento ouvir que seu pedido está atrasado.\n",
            "\n",
            "Para ...\u001b[0m\n",
            "Lamento ouvir que seu pedido está atrasado.\n",
            "\n",
            "Para que eu possa verificar o que aconteceu e te dar uma atualização, por favor, me informe o número do seu pedido.\n",
            "\u001b[32m21:17:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[atendimento]\u001b[0m | \u001b[1m💬 Chat: Contexto: ['Meu pedido está atrasado.', 'Lamento o... → Para que eu possa verificar o status da sua entreg...\u001b[0m\n",
            "Para que eu possa verificar o status da sua entrega e te dar uma atualização, por favor, me informe o número do seu pedido.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Classe que executa análise de sentimentos em lote\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class AnaliseSentimentos:\n",
        "    def __init__(self, agent_id=\"sentimentos\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def analisar_lote(self, textos):\n",
        "        resultados = []\n",
        "        for texto in textos:\n",
        "            prompt = f\"Classifique o sentimento (positivo, negativo, neutro) do seguinte texto:\\n{texto}\"\n",
        "            resultados.append(self.agent.chat(prompt))\n",
        "        return resultados\n",
        "\n",
        "comentarios = [\n",
        "    \"O produto é excelente! Mas faltou um engrediente\",\n",
        "    \"Não gostei do atendimento.\",\n",
        "    \"Entrega dentro do prazo.\"\n",
        "]\n",
        "analista = AnaliseSentimentos()\n",
        "print(analista.analisar_lote(comentarios))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "LH05NznbyzIg",
        "outputId": "f98d3ab2-e2b8-41bc-e824-2545e237607c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m21:19:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[sentimentos]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: sentimentos, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m21:19:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[sentimentos]\u001b[0m | \u001b[1m💬 Chat: Classifique o sentimento (positivo, negativo, neut... → Positivo...\u001b[0m\n",
            "\u001b[32m21:19:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[sentimentos]\u001b[0m | \u001b[1m💬 Chat: Classifique o sentimento (positivo, negativo, neut... → Negativo...\u001b[0m\n",
            "\u001b[32m21:19:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[sentimentos]\u001b[0m | \u001b[1m💬 Chat: Classifique o sentimento (positivo, negativo, neut... → Neutro...\u001b[0m\n",
            "['Positivo', 'Negativo', 'Neutro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Classe para geração automatizada de documentação técnica de código\n",
        "from mangaba_ai import MangabaAgent\n",
        "\n",
        "class GeradorDocumentacao:\n",
        "    def __init__(self, agent_id=\"docgen\"):\n",
        "        self.agent = MangabaAgent(model=\"gemini-2.5-flash\", agent_id=agent_id)\n",
        "\n",
        "    def documentar_codigo(self, codigo):\n",
        "        prompt = f\"Leia o código abaixo e gere uma documentação técnica detalhada, incluindo exemplos de uso:\\n{codigo}\"\n",
        "        return self.agent.chat(prompt)\n",
        "\n",
        "codigo = '''\n",
        "def calcular_media(valores):\n",
        "    \"\"\"Recebe uma lista de números e retorna a média aritmética.\"\"\"\n",
        "    return sum(valores) / len(valores)\n",
        "'''\n",
        "docgen = GeradorDocumentacao()\n",
        "print(docgen.documentar_codigo(codigo))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dWvb87inzRDX",
        "outputId": "18ab9179-2871-4249-c04c-7a32e2dfd19e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m21:19:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[docgen]\u001b[0m | \u001b[1m✅ Agente inicializado - ID: docgen, Modelo: gemini-2.5-flash\u001b[0m\n",
            "\u001b[32m21:20:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mMangabaAgent[docgen]\u001b[0m | \u001b[1m💬 Chat: Leia o código abaixo e gere uma documentação técni... → Claro! Abaixo está a documentação técnica detalhad...\u001b[0m\n",
            "Claro! Abaixo está a documentação técnica detalhada para a função `calcular_media`, incluindo exemplos de uso.\n",
            "\n",
            "---\n",
            "\n",
            "# Documentação Técnica da Função `calcular_media`\n",
            "\n",
            "## 1. Visão Geral\n",
            "\n",
            "A função `calcular_media` é uma utilidade simples e direta projetada para calcular a média aritmética de uma coleção de números. Ela recebe uma lista (ou qualquer iterável) de valores numéricos e retorna a soma desses valores dividida pela quantidade de elementos. É ideal para cenários onde é necessário obter um valor central representativo de um conjunto de dados.\n",
            "\n",
            "## 2. Assinatura da Função\n",
            "\n",
            "```python\n",
            "def calcular_media(valores):\n",
            "```\n",
            "\n",
            "## 3. Parâmetros\n",
            "\n",
            "*   **`valores`** (obrigatório)\n",
            "    *   **Tipo:** `list` (ou qualquer iterável, como `tuple`, `set`, etc.)\n",
            "    *   **Descrição:** Uma coleção de números (inteiros ou decimais) dos quais a média será calculada.\n",
            "    *   **Restrições:**\n",
            "        *   Todos os elementos dentro do iterável devem ser do tipo numérico (`int` ou `float`).\n",
            "        *   **Não deve ser uma lista vazia.** Se uma lista vazia for fornecida, a função levantará uma exceção `ZeroDivisionError`.\n",
            "\n",
            "## 4. Retorno\n",
            "\n",
            "*   **Tipo:** `float`\n",
            "*   **Descrição:** A média aritmética dos números fornecidos. O resultado é sempre um número de ponto flutuante, mesmo que todos os valores de entrada sejam inteiros (devido à operação de divisão).\n",
            "\n",
            "## 5. Exceções / Erros Potenciais\n",
            "\n",
            "*   **`ZeroDivisionError`**\n",
            "    *   **Causa:** Esta exceção ocorrerá se a lista `valores` for vazia (ou seja, `len(valores)` for 0). A divisão por zero não é permitida em Python e resultará neste erro.\n",
            "    *   **Como evitar/tratar:** É responsabilidade do chamador garantir que a lista de entrada não esteja vazia antes de chamar a função. Uma verificação condicional (`if valores:`) ou um bloco `try-except` pode ser usado para lidar com este cenário.\n",
            "\n",
            "## 6. Exemplos de Uso\n",
            "\n",
            "### Exemplo 1: Calculando a média de uma lista de inteiros\n",
            "\n",
            "```python\n",
            "# Lista de números inteiros\n",
            "numeros_inteiros = [10, 20, 30, 40, 50]\n",
            "\n",
            "# Chamando a função\n",
            "media_inteiros = calcular_media(numeros_inteiros)\n",
            "\n",
            "# Imprimindo o resultado\n",
            "print(f\"A média dos números {numeros_inteiros} é: {media_inteiros}\")\n",
            "# Saída esperada: A média dos números [10, 20, 30, 40, 50] é: 30.0\n",
            "```\n",
            "\n",
            "### Exemplo 2: Calculando a média de uma lista de números decimais\n",
            "\n",
            "```python\n",
            "# Lista de notas (números decimais)\n",
            "notas = [7.5, 8.0, 6.5, 9.0, 7.0]\n",
            "\n",
            "# Chamando a função\n",
            "media_notas = calcular_media(notas)\n",
            "\n",
            "# Imprimindo o resultado\n",
            "print(f\"A média das notas {notas} é: {media_notas}\")\n",
            "# Saída esperada: A média das notas [7.5, 8.0, 6.5, 9.0, 7.0] é: 7.6\n",
            "```\n",
            "\n",
            "### Exemplo 3: Calculando a média de uma tupla de números\n",
            "\n",
            "```python\n",
            "# Tupla de dados\n",
            "dados_tupla = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            "# Chamando a função\n",
            "media_tupla = calcular_media(dados_tupla)\n",
            "\n",
            "# Imprimindo o resultado\n",
            "print(f\"A média dos dados na tupla {dados_tupla} é: {media_tupla}\")\n",
            "# Saída esperada: A média dos dados na tupla (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) é: 5.5\n",
            "```\n",
            "\n",
            "### Exemplo 4: Tratamento de erro para lista vazia\n",
            "\n",
            "```python\n",
            "# Lista vazia\n",
            "lista_vazia = []\n",
            "\n",
            "try:\n",
            "    # Tentando calcular a média de uma lista vazia\n",
            "    media_vazia = calcular_media(lista_vazia)\n",
            "    print(f\"A média da lista vazia é: {media_vazia}\")\n",
            "except ZeroDivisionError:\n",
            "    # Capturando e tratando o erro\n",
            "    print(\"Erro: Não é possível calcular a média de uma lista vazia. Por favor, forneça uma lista com pelo menos um elemento.\")\n",
            "# Saída esperada: Erro: Não é possível calcular a média de uma lista vazia. Por favor, forneça uma lista com pelo menos um elemento.\n",
            "```\n",
            "\n",
            "### Exemplo 5: Calculando a média com um único elemento\n",
            "\n",
            "```python\n",
            "# Lista com um único valor\n",
            "valor_unico = [42]\n",
            "\n",
            "# Chamando a função\n",
            "media_unica = calcular_media(valor_unico)\n",
            "\n",
            "# Imprimindo o resultado\n",
            "print(f\"A média do valor único {valor_unico} é: {media_unica}\")\n",
            "# Saída esperada: A média do valor único [42] é: 42.0\n",
            "```\n",
            "\n",
            "## 7. Considerações e Boas Práticas\n",
            "\n",
            "*   **Validação de Entrada:** A função atual assume que `valores` contém apenas números. Para maior robustez em um ambiente de produção, seria prudente adicionar validação interna para verificar se cada elemento do iterável é realmente numérico. Caso contrário, poderia ocorrer um `TypeError` ao tentar somar tipos não numéricos.\n",
            "*   **Tratamento de Listas Vazias:** Embora o `ZeroDivisionError` seja capturável, em algumas aplicações, pode ser desejável que a função retorne um valor específico (como `0` ou `None`) para uma lista vazia, ou que levante um `ValueError` mais descritivo em vez de um `ZeroDivisionError`.\n",
            "    *   **Exemplo de melhoria com validação e type hinting:**\n",
            "        ```python\n",
            "        from typing import List, Union\n",
            "\n",
            "        def calcular_media_aprimorada(valores: List[Union[int, float]]) -> float:\n",
            "            \"\"\"\n",
            "            Recebe uma lista de números e retorna a média aritmética.\n",
            "            Levanta ValueError se a lista estiver vazia ou contiver não-números.\n",
            "            \"\"\"\n",
            "            if not valores:\n",
            "                raise ValueError(\"A lista de valores não pode ser vazia para calcular a média.\")\n",
            "\n",
            "            for valor in valores:\n",
            "                if not isinstance(valor, (int, float)):\n",
            "                    raise TypeError(f\"Todos os elementos devem ser números (int ou float). Encontrado: {type(valor).__name__}\")\n",
            "\n",
            "            return sum(valores) / len(valores)\n",
            "        ```\n",
            "*   **Desempenho:** Para listas de tamanho moderado, o uso das funções built-in `sum()` e `len()` é altamente otimizado e eficiente. Para listas extremamente grandes, a performance será proporcional ao número de elementos, mas ainda assim muito boa devido à implementação em C dessas funções.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro! Abaixo está a documentação técnica detalhada para a função `calcular_media`, incluindo exemplos de uso.\n",
        "\n",
        "---\n",
        "\n",
        "# Documentação Técnica da Função `calcular_media`\n",
        "\n",
        "## 1. Visão Geral\n",
        "\n",
        "A função `calcular_media` é uma utilidade simples e direta projetada para calcular a média aritmética de uma coleção de números. Ela recebe uma lista (ou qualquer iterável) de valores numéricos e retorna a soma desses valores dividida pela quantidade de elementos. É ideal para cenários onde é necessário obter um valor central representativo de um conjunto de dados.\n",
        "\n",
        "## 2. Assinatura da Função\n",
        "\n",
        "```python\n",
        "def calcular_media(valores):\n",
        "```\n",
        "\n",
        "## 3. Parâmetros\n",
        "\n",
        "*   **`valores`** (obrigatório)\n",
        "    *   **Tipo:** `list` (ou qualquer iterável, como `tuple`, `set`, etc.)\n",
        "    *   **Descrição:** Uma coleção de números (inteiros ou decimais) dos quais a média será calculada.\n",
        "    *   **Restrições:**\n",
        "        *   Todos os elementos dentro do iterável devem ser do tipo numérico (`int` ou `float`).\n",
        "        *   **Não deve ser uma lista vazia.** Se uma lista vazia for fornecida, a função levantará uma exceção `ZeroDivisionError`.\n",
        "\n",
        "## 4. Retorno\n",
        "\n",
        "*   **Tipo:** `float`\n",
        "*   **Descrição:** A média aritmética dos números fornecidos. O resultado é sempre um número de ponto flutuante, mesmo que todos os valores de entrada sejam inteiros (devido à operação de divisão).\n",
        "\n",
        "## 5. Exceções / Erros Potenciais\n",
        "\n",
        "*   **`ZeroDivisionError`**\n",
        "    *   **Causa:** Esta exceção ocorrerá se a lista `valores` for vazia (ou seja, `len(valores)` for 0). A divisão por zero não é permitida em Python e resultará neste erro.\n",
        "    *   **Como evitar/tratar:** É responsabilidade do chamador garantir que a lista de entrada não esteja vazia antes de chamar a função. Uma verificação condicional (`if valores:`) ou um bloco `try-except` pode ser usado para lidar com este cenário.\n",
        "\n",
        "## 6. Exemplos de Uso\n",
        "\n",
        "### Exemplo 1: Calculando a média de uma lista de inteiros\n",
        "\n",
        "```python\n",
        "# Lista de números inteiros\n",
        "numeros_inteiros = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Chamando a função\n",
        "media_inteiros = calcular_media(numeros_inteiros)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(f\"A média dos números {numeros_inteiros} é: {media_inteiros}\")\n",
        "# Saída esperada: A média dos números [10, 20, 30, 40, 50] é: 30.0\n",
        "```\n",
        "\n",
        "### Exemplo 2: Calculando a média de uma lista de números decimais\n",
        "\n",
        "```python\n",
        "# Lista de notas (números decimais)\n",
        "notas = [7.5, 8.0, 6.5, 9.0, 7.0]\n",
        "\n",
        "# Chamando a função\n",
        "media_notas = calcular_media(notas)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(f\"A média das notas {notas} é: {media_notas}\")\n",
        "# Saída esperada: A média das notas [7.5, 8.0, 6.5, 9.0, 7.0] é: 7.6\n",
        "```\n",
        "\n",
        "### Exemplo 3: Calculando a média de uma tupla de números\n",
        "\n",
        "```python\n",
        "# Tupla de dados\n",
        "dados_tupla = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
        "\n",
        "# Chamando a função\n",
        "media_tupla = calcular_media(dados_tupla)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(f\"A média dos dados na tupla {dados_tupla} é: {media_tupla}\")\n",
        "# Saída esperada: A média dos dados na tupla (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) é: 5.5\n",
        "```\n",
        "\n",
        "### Exemplo 4: Tratamento de erro para lista vazia\n",
        "\n",
        "```python\n",
        "# Lista vazia\n",
        "lista_vazia = []\n",
        "\n",
        "try:\n",
        "    # Tentando calcular a média de uma lista vazia\n",
        "    media_vazia = calcular_media(lista_vazia)\n",
        "    print(f\"A média da lista vazia é: {media_vazia}\")\n",
        "except ZeroDivisionError:\n",
        "    # Capturando e tratando o erro\n",
        "    print(\"Erro: Não é possível calcular a média de uma lista vazia. Por favor, forneça uma lista com pelo menos um elemento.\")\n",
        "# Saída esperada: Erro: Não é possível calcular a média de uma lista vazia. Por favor, forneça uma lista com pelo menos um elemento.\n",
        "```\n",
        "\n",
        "### Exemplo 5: Calculando a média com um único elemento\n",
        "\n",
        "```python\n",
        "# Lista com um único valor\n",
        "valor_unico = [42]\n",
        "\n",
        "# Chamando a função\n",
        "media_unica = calcular_media(valor_unico)\n",
        "\n",
        "# Imprimindo o resultado\n",
        "print(f\"A média do valor único {valor_unico} é: {media_unica}\")\n",
        "# Saída esperada: A média do valor único [42] é: 42.0\n",
        "```\n",
        "\n",
        "## 7. Considerações e Boas Práticas\n",
        "\n",
        "*   **Validação de Entrada:** A função atual assume que `valores` contém apenas números. Para maior robustez em um ambiente de produção, seria prudente adicionar validação interna para verificar se cada elemento do iterável é realmente numérico. Caso contrário, poderia ocorrer um `TypeError` ao tentar somar tipos não numéricos.\n",
        "*   **Tratamento de Listas Vazias:** Embora o `ZeroDivisionError` seja capturável, em algumas aplicações, pode ser desejável que a função retorne um valor específico (como `0` ou `None`) para uma lista vazia, ou que levante um `ValueError` mais descritivo em vez de um `ZeroDivisionError`.\n",
        "    *   **Exemplo de melhoria com validação e type hinting:**\n",
        "        ```python\n",
        "        from typing import List, Union\n",
        "\n",
        "        def calcular_media_aprimorada(valores: List[Union[int, float]]) -> float:\n",
        "            \"\"\"\n",
        "            Recebe uma lista de números e retorna a média aritmética.\n",
        "            Levanta ValueError se a lista estiver vazia ou contiver não-números.\n",
        "            \"\"\"\n",
        "            if not valores:\n",
        "                raise ValueError(\"A lista de valores não pode ser vazia para calcular a média.\")\n",
        "\n",
        "            for valor in valores:\n",
        "                if not isinstance(valor, (int, float)):\n",
        "                    raise TypeError(f\"Todos os elementos devem ser números (int ou float). Encontrado: {type(valor).__name__}\")\n",
        "\n",
        "            return sum(valores) / len(valores)\n",
        "        ```\n",
        "*   **Desempenho:** Para listas de tamanho moderado, o uso das funções built-in `sum()` e `len()` é altamente otimizado e eficiente. Para listas extremamente grandes, a performance será proporcional ao número de elementos, mas ainda assim muito boa devido à implementação em C dessas funções."
      ],
      "metadata": {
        "id": "GpAh2EVMznR5"
      }
    }
  ]
}